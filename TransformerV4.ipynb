{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trickshotblaster/nn-practices/blob/main/TransformerV4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "42bb70b4-42db-4321-e0cd-4531b19f6165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt2\")\n",
        "print(enc.n_vocab)\n",
        "enc.decode(enc.encode(\"Hello world!\"))"
      ],
      "metadata": {
        "id": "lIZZax_FG4oj",
        "outputId": "f741c8aa-e069-4c70-98ae-9b8feea02b1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50257\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello world!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f: # input.txt\n",
        "    text = f.read()\n",
        "print(text[:100])"
      ],
      "metadata": {
        "id": "v4_640nZH6ze",
        "outputId": "da09343a-58ab-4fa9-fc70-8a06a0dd1b4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-11 21:49:35--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-06-11 21:49:35 (26.5 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_amount = 0.95\n",
        "idx = int(train_amount * len(text))\n",
        "train_text = text[:idx]\n",
        "val_text = text[idx:]"
      ],
      "metadata": {
        "id": "HG6tD8arIGqu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "xFvovfpvaNg_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Z55xQhpY21KG"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "  def __init__(self, text, batch_size, block_size):\n",
        "    self.text = torch.tensor(enc.encode(text)).to(device)\n",
        "    self.batch_size = batch_size\n",
        "    self.block_size = block_size\n",
        "    self.current_pos = 0\n",
        "  def steps_per_epoch(self):\n",
        "    return len(self.text) // (self.batch_size * self.block_size)\n",
        "  def next(self):\n",
        "    if self.current_pos + self.batch_size * self.block_size + 1 >= len(self.text):\n",
        "      self.current_pos = 0\n",
        "    buf = self.text[self.current_pos:self.current_pos + self.batch_size * self.block_size + 1]\n",
        "    ins = buf[:-1].view(self.batch_size, self.block_size)\n",
        "    tgts = buf[1:].view(self.batch_size, self.block_size)\n",
        "    self.current_pos += self.batch_size * self.block_size + 1\n",
        "    return ins, tgts\n",
        "dl = DataLoader(train_text, 4, 8)\n",
        "dl.next()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJN-mDlGIW_N",
        "outputId": "ea0cfb52-8cc8-4914-d778-02d2f2419c7e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 5962, 22307,    25,   198,  8421,   356,  5120,   597],\n",
              "         [ 2252,    11,  3285,   502,  2740,    13,   198,   198],\n",
              "         [ 3237,    25,   198,  5248,   461,    11,  2740,    13],\n",
              "         [  198,   198,  5962, 22307,    25,   198,  1639,   389]],\n",
              "        device='cuda:0'),\n",
              " tensor([[22307,    25,   198,  8421,   356,  5120,   597,  2252],\n",
              "         [   11,  3285,   502,  2740,    13,   198,   198,  3237],\n",
              "         [   25,   198,  5248,   461,    11,  2740,    13,   198],\n",
              "         [  198,  5962, 22307,    25,   198,  1639,   389,   477]],\n",
              "        device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "    self.d_key = self.d_model // self.n_heads\n",
        "\n",
        "    self.wq = nn.Linear(d_model, d_model)\n",
        "    self.wk = nn.Linear(d_model, d_model)\n",
        "    self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.wo = nn.Linear(d_model, d_model)\n",
        "  def forward(self, ins, mask=None):\n",
        "    batch_size, seq_len, d_model = ins.size()\n",
        "    Q = self.wq(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "    K = self.wk(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "    V = self.wv(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "\n",
        "    scaled_dot_product = (Q @ K.transpose(2, 3)) / (self.d_model ** 0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "      scaled_dot_product += mask\n",
        "\n",
        "    attn_scores = F.softmax(scaled_dot_product, dim=-1) @ V\n",
        "    attn_scores = attn_scores.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "    return self.wo(attn_scores)\n",
        "MHA = MultiHeadAttention(32, 4)\n",
        "MHA(torch.randn(2, 16, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPtZtDsYakHh",
        "outputId": "b46d065c-db91-4d25-ad9e-d0f51edb1b3c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1465, -0.0072, -0.1859,  ...,  0.0988,  0.1258,  0.1897],\n",
              "         [ 0.1164, -0.0511, -0.1894,  ...,  0.0652,  0.1090,  0.1570],\n",
              "         [ 0.1068, -0.0378, -0.1796,  ...,  0.0628,  0.1309,  0.1662],\n",
              "         ...,\n",
              "         [ 0.1485, -0.0119, -0.1950,  ...,  0.0917,  0.1187,  0.1919],\n",
              "         [ 0.1222, -0.0181, -0.1773,  ...,  0.0908,  0.1043,  0.1626],\n",
              "         [ 0.1329, -0.0275, -0.1921,  ...,  0.0852,  0.1044,  0.1713]],\n",
              "\n",
              "        [[ 0.1677,  0.0359, -0.1063,  ...,  0.1061,  0.2597,  0.1273],\n",
              "         [ 0.1588,  0.0244, -0.0693,  ...,  0.1145,  0.2435,  0.1713],\n",
              "         [ 0.1374,  0.0180, -0.0611,  ...,  0.1227,  0.2743,  0.1127],\n",
              "         ...,\n",
              "         [ 0.1540,  0.0666, -0.0622,  ...,  0.0823,  0.2676,  0.1201],\n",
              "         [ 0.1591,  0.0413, -0.0641,  ...,  0.0839,  0.2593,  0.0880],\n",
              "         [ 0.1709,  0.0635, -0.0681,  ...,  0.0869,  0.2646,  0.0714]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_size, hidden_size, out_size):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(in_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, out_size)\n",
        "    self.gelu = nn.GELU()\n",
        "  def forward(self, ins):\n",
        "    acts = self.gelu(self.l1(ins))\n",
        "    return self.l2(acts)"
      ],
      "metadata": {
        "id": "JcJ9yLT9hRXb"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_heads, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.MHA = MultiHeadAttention(d_model, n_heads)\n",
        "    self.MLP = MLP(d_model, 4*d_model, d_model)\n",
        "    self.layernorm1 = nn.LayerNorm(d_model)\n",
        "    self.layernorm2 = nn.LayerNorm(d_model)\n",
        "  def forward(self, ins, mask=None):\n",
        "    res1 = ins.clone()\n",
        "    attn_result = self.MHA(ins, mask=mask)\n",
        "    norm_result = self.layernorm1(attn_result)\n",
        "    norm_result += res1\n",
        "    res2 = norm_result.clone()\n",
        "    mlp_result = self.MLP(norm_result)\n",
        "    mlp_result_norm = self.layernorm2(mlp_result)\n",
        "    return mlp_result_norm + res2"
      ],
      "metadata": {
        "id": "pJ_EKzsag25J"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, n_layers=2, n_heads=4, d_model=64):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.block_size = block_size\n",
        "    self.n_layers = n_layers\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.position_embedding = nn.Embedding(block_size, d_model)\n",
        "    self.decoder_stack = nn.ModuleList([\n",
        "        DecoderBlock(vocab_size, d_model, n_heads) for _ in range(n_layers)\n",
        "    ])\n",
        "    self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "    #self.output_proj.weight = self.token_embedding.weight\n",
        "  def forward(self, ins, targets=None):\n",
        "    B, T = ins.size()\n",
        "\n",
        "    x = self.token_embedding(ins.to(device))\n",
        "    input_indices = torch.arange(T).to(device)\n",
        "    x += self.position_embedding(input_indices)\n",
        "\n",
        "    look_ahead_mask = torch.triu(\n",
        "        torch.ones((T, T)), diagonal=1\n",
        "    )\n",
        "    look_ahead_mask.masked_fill_(look_ahead_mask == 1, float(\"-inf\"))\n",
        "    look_ahead_mask = look_ahead_mask.to(device)\n",
        "\n",
        "    for decoder in self.decoder_stack:\n",
        "      x = decoder(x, mask=look_ahead_mask)\n",
        "    logits = self.output_proj(x)\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      targets = targets.to(device)\n",
        "      loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "    return logits, loss\n",
        "my_GPT = GPT(enc.n_vocab, 32, 12, 12, 768).to(device)"
      ],
      "metadata": {
        "id": "3dsbJ4B5aY19"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = dl.next()\n",
        "logits, loss = my_GPT(x, y)\n",
        "print(logits.shape, loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ6HIDlREtJy",
        "outputId": "2931230d-7e2c-464e-ee07-5796dd4e31fa"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 50257]) 14.276529312133789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "block_size = 32\n",
        "n_layers = 4\n",
        "n_heads = 4\n",
        "d_model = 128\n",
        "lr = 3e-4\n",
        "\n",
        "\n",
        "\n",
        "my_GPT = GPT(enc.n_vocab, block_size, n_layers, n_heads, d_model)\n",
        "my_GPT = my_GPT.to(device)\n",
        "\n",
        "compile = True\n",
        "if compile and torch.cuda.is_available():\n",
        "  my_GPT = torch.compile(my_GPT)\n",
        "\n",
        "optim = torch.optim.AdamW(my_GPT.parameters(), lr=lr)\n",
        "data_loader = DataLoader(train_text, batch_size, block_size)\n",
        "\n",
        "val_data_loader = DataLoader(val_text, batch_size, block_size)\n",
        "val_interval = 200\n",
        "\n",
        "log_interval = 50\n",
        "max_steps = 3000\n",
        "print(\"Steps per epoch:\", data_loader.steps_per_epoch())\n",
        "print(f\"GPT Parameters: {sum(p.numel() for p in my_GPT.parameters()) / 1e6} million\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MyAbAJw_DlZ",
        "outputId": "e058f322-d3e1-4ab3-d46b-0ccdb5f67a67"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps per epoch: 156\n",
            "GPT Parameters: 13.713233 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"high\")"
      ],
      "metadata": {
        "id": "2U2yA8tmw5og"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "best_val_loss = float(\"inf\")\n",
        "my_GPT.train()\n",
        "for step in range(max_steps + 1):\n",
        "  step_start = time.time()\n",
        "  x, y = data_loader.next()\n",
        "  logits, loss = my_GPT(x, y)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print(f\"Step {step}, loss: {loss.item()}, time: {round((time.time() - step_start) * 1e3, 2)} ms\")\n",
        "  if step % val_interval == 0:\n",
        "    with torch.no_grad():\n",
        "      val_loss = 0\n",
        "      for val_step in range(val_data_loader.steps_per_epoch()):\n",
        "        val_x, val_y = val_data_loader.next()\n",
        "        logits, loss = my_GPT(val_x, val_y)\n",
        "        val_loss += loss\n",
        "      val_loss /= val_data_loader.steps_per_epoch()\n",
        "      if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(my_GPT.state_dict(), 'best_model.pth')\n",
        "      print(f\"Val loss for step {step}: {val_loss}\")\n",
        "my_GPT.load_state_dict(torch.load('best_model.pth'))\n",
        "my_GPT.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDzsArmPAR-B",
        "outputId": "9b4e7e9e-d7f4-438f-8b3b-66d76f7c9596"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, loss: 12.45822525024414, time: 21222.53 ms\n",
            "Val loss for step 0: 11.486330032348633\n",
            "Step 50, loss: 7.174983024597168, time: 221.92 ms\n",
            "Step 100, loss: 6.1902241706848145, time: 223.84 ms\n",
            "Step 150, loss: 6.5473480224609375, time: 222.98 ms\n",
            "Step 200, loss: 5.959547996520996, time: 224.23 ms\n",
            "Val loss for step 200: 6.103331089019775\n",
            "Step 250, loss: 5.141306400299072, time: 223.82 ms\n",
            "Step 300, loss: 5.676750183105469, time: 226.76 ms\n",
            "Step 350, loss: 5.078444004058838, time: 223.62 ms\n",
            "Step 400, loss: 5.461614608764648, time: 227.33 ms\n",
            "Val loss for step 400: 5.703752517700195\n",
            "Step 450, loss: 4.707574367523193, time: 228.5 ms\n",
            "Step 500, loss: 4.616297721862793, time: 226.37 ms\n",
            "Step 550, loss: 5.273550987243652, time: 226.83 ms\n",
            "Step 600, loss: 5.044036865234375, time: 228.69 ms\n",
            "Val loss for step 600: 5.409703731536865\n",
            "Step 650, loss: 4.630270957946777, time: 226.94 ms\n",
            "Step 700, loss: 4.664117813110352, time: 227.15 ms\n",
            "Step 750, loss: 4.572318077087402, time: 225.87 ms\n",
            "Step 800, loss: 4.75049352645874, time: 226.48 ms\n",
            "Val loss for step 800: 5.333583354949951\n",
            "Step 850, loss: 4.333022117614746, time: 225.48 ms\n",
            "Step 900, loss: 4.595035552978516, time: 225.24 ms\n",
            "Step 950, loss: 4.193530559539795, time: 225.17 ms\n",
            "Step 1000, loss: 4.237274646759033, time: 222.13 ms\n",
            "Val loss for step 1000: 5.359527111053467\n",
            "Step 1050, loss: 4.724485397338867, time: 223.43 ms\n",
            "Step 1100, loss: 4.268874168395996, time: 223.21 ms\n",
            "Step 1150, loss: 4.296621799468994, time: 222.22 ms\n",
            "Step 1200, loss: 4.459567070007324, time: 222.72 ms\n",
            "Val loss for step 1200: 5.299712181091309\n",
            "Step 1250, loss: 4.086124420166016, time: 222.52 ms\n",
            "Step 1300, loss: 4.29354190826416, time: 224.24 ms\n",
            "Step 1350, loss: 3.6272072792053223, time: 222.01 ms\n",
            "Step 1400, loss: 4.010711193084717, time: 224.79 ms\n",
            "Val loss for step 1400: 5.137734889984131\n",
            "Step 1450, loss: 3.956054449081421, time: 220.22 ms\n",
            "Step 1500, loss: 3.917675495147705, time: 224.0 ms\n",
            "Step 1550, loss: 3.6354193687438965, time: 221.71 ms\n",
            "Step 1600, loss: 4.040669918060303, time: 222.65 ms\n",
            "Val loss for step 1600: 5.255364418029785\n",
            "Step 1650, loss: 3.8040995597839355, time: 222.77 ms\n",
            "Step 1700, loss: 3.5124995708465576, time: 224.28 ms\n",
            "Step 1750, loss: 3.4938740730285645, time: 225.45 ms\n",
            "Step 1800, loss: 3.8489813804626465, time: 223.38 ms\n",
            "Val loss for step 1800: 5.305763244628906\n",
            "Step 1850, loss: 3.803971290588379, time: 225.32 ms\n",
            "Step 1900, loss: 3.7115752696990967, time: 224.35 ms\n",
            "Step 1950, loss: 3.4773988723754883, time: 225.0 ms\n",
            "Step 2000, loss: 3.347242832183838, time: 227.11 ms\n",
            "Val loss for step 2000: 5.285120964050293\n",
            "Step 2050, loss: 3.546888589859009, time: 225.37 ms\n",
            "Step 2100, loss: 3.470076560974121, time: 223.15 ms\n",
            "Step 2150, loss: 3.790174722671509, time: 224.96 ms\n",
            "Step 2200, loss: 3.580355405807495, time: 226.24 ms\n",
            "Val loss for step 2200: 5.367863178253174\n",
            "Step 2250, loss: 3.721226930618286, time: 225.58 ms\n",
            "Step 2300, loss: 3.5971741676330566, time: 224.95 ms\n",
            "Step 2350, loss: 3.4087777137756348, time: 222.92 ms\n",
            "Step 2400, loss: 3.3799052238464355, time: 223.35 ms\n",
            "Val loss for step 2400: 5.498826503753662\n",
            "Step 2450, loss: 3.710653305053711, time: 226.07 ms\n",
            "Step 2500, loss: 3.4674465656280518, time: 224.61 ms\n",
            "Step 2550, loss: 3.304715156555176, time: 224.93 ms\n",
            "Step 2600, loss: 3.5454256534576416, time: 224.72 ms\n",
            "Val loss for step 2600: 5.535457134246826\n",
            "Step 2650, loss: 2.9197049140930176, time: 220.52 ms\n",
            "Step 2700, loss: 3.35614013671875, time: 226.6 ms\n",
            "Step 2750, loss: 3.140899658203125, time: 223.37 ms\n",
            "Step 2800, loss: 2.873342514038086, time: 225.27 ms\n",
            "Val loss for step 2800: 5.5000152587890625\n",
            "Step 2850, loss: 2.8428659439086914, time: 222.85 ms\n",
            "Step 2900, loss: 3.1829683780670166, time: 222.04 ms\n",
            "Step 2950, loss: 3.381812334060669, time: 224.19 ms\n",
            "Step 3000, loss: 3.165464401245117, time: 223.36 ms\n",
            "Val loss for step 3000: 5.664863109588623\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OptimizedModule(\n",
              "  (_orig_mod): GPT(\n",
              "    (token_embedding): Embedding(50257, 128)\n",
              "    (position_embedding): Embedding(32, 128)\n",
              "    (decoder_stack): ModuleList(\n",
              "      (0-3): 4 x DecoderBlock(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (MHA): MultiHeadAttention(\n",
              "          (wq): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (wk): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (wv): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (wo): Linear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (MLP): MLP(\n",
              "          (l1): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (l2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "        )\n",
              "        (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (output_proj): Linear(in_features=128, out_features=50257, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\\n\"\n",
        "input_tokens = enc.encode(prompt)\n",
        "output_tokens = enc.encode(prompt)\n",
        "for x in range(200):\n",
        "  if len(input_tokens) > block_size:\n",
        "    input_tokens = input_tokens[1:]\n",
        "  context_tensor = torch.tensor(input_tokens).view(1, -1).to(device)\n",
        "  logits, loss = my_GPT(context_tensor)\n",
        "  probs = F.softmax(logits[:, -1, :])\n",
        "  result = torch.multinomial(probs, num_samples=1).item()\n",
        "  input_tokens.append(result)\n",
        "  output_tokens.append(result)\n",
        "print(enc.decode(output_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn5PDMKwdvvF",
        "outputId": "d382f6fe-a585-4f1e-d781-4692b08732d1"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-72-814a6677e104>:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = F.softmax(logits[:, -1, :])\n",
            "<ipython-input-72-814a6677e104>:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = F.softmax(logits[:, -1, :])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "HORTENSIO:\n",
            "Tush she Isabel; rewards our heart yourself to death,\n",
            "Peace, proud silence for your words, his son,\n",
            " templeemark me begun look to you; ha' pleasure to Brittany.\n",
            "That's love that all mal speech?\n",
            "\n",
            "PETRUCHIO:\n",
            "Who burst?\n",
            "\n",
            " presenceUMIO:\n",
            "Be in Pad to his whore require:\n",
            "Wriceardine by three thou hastUS:\n",
            "Yes unto these woman better the ins the wholeness.\n",
            "\n",
            "HORTENSIO:\n",
            "Hark! if or good time shoulder, and brought for joy:\n",
            "fl sun judge me to bed to Barthie:\n",
            "Onea enforcedciuscear chast Brut sister.\n",
            "\n",
            "TRANIO:\n",
            "Why, I'll firmly the unknown steel of a great host.\n",
            "\n",
            "POMPEY:\n",
            "That are happily, that on this is but he is with how plain\n",
            "If it how Katharina: think,\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}