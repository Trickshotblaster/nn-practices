{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trickshotblaster/nn-practices/blob/main/TransformerV4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "ab1648ec-f4be-45ea-ee9c-2acb88866dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt2\")\n",
        "print(enc.n_vocab)\n",
        "enc.decode(enc.encode(\"Hello world!\"))"
      ],
      "metadata": {
        "id": "lIZZax_FG4oj",
        "outputId": "6bf50fee-cf07-4ec0-de54-94097c3954b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50257\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello world!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[:100])"
      ],
      "metadata": {
        "id": "v4_640nZH6ze",
        "outputId": "94dc9f6d-f1c3-48f2-8f67-46c06ff33fbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-11 15:33:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-06-11 15:33:13 (25.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_amount = 0.95\n",
        "idx = int(train_amount * len(text))\n",
        "train_text = text[:idx]\n",
        "val_text = text[idx:]"
      ],
      "metadata": {
        "id": "HG6tD8arIGqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "xFvovfpvaNg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "  def __init__(self, text, batch_size, block_size):\n",
        "    self.text = enc.encode(text)\n",
        "    self.batch_size = batch_size\n",
        "    self.block_size = block_size\n",
        "    self.current_pos = 0\n",
        "  def steps_per_epoch(self):\n",
        "    return len(self.text) // (self.batch_size * self.block_size)\n",
        "  def next(self):\n",
        "    if self.current_pos + self.batch_size * self.block_size + 1 >= len(self.text):\n",
        "      self.current_pos = 0\n",
        "    buf = self.text[self.current_pos:self.current_pos + self.batch_size * self.block_size + 1]\n",
        "    ins = torch.tensor(buf[:-1]).view(self.batch_size, self.block_size)\n",
        "    tgts = torch.tensor(buf[1:]).view(self.batch_size, self.block_size)\n",
        "    self.current_pos += self.batch_size * self.block_size + 1\n",
        "    return ins, tgts\n",
        "dl = DataLoader(train_text, 4, 8)\n",
        "dl.next()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJN-mDlGIW_N",
        "outputId": "40443b21-5b0b-44a5-f71b-7722354b74b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 5962, 22307,    25,   198,  8421,   356,  5120,   597],\n",
              "         [ 2252,    11,  3285,   502,  2740,    13,   198,   198],\n",
              "         [ 3237,    25,   198,  5248,   461,    11,  2740,    13],\n",
              "         [  198,   198,  5962, 22307,    25,   198,  1639,   389]]),\n",
              " tensor([[22307,    25,   198,  8421,   356,  5120,   597,  2252],\n",
              "         [   11,  3285,   502,  2740,    13,   198,   198,  3237],\n",
              "         [   25,   198,  5248,   461,    11,  2740,    13,   198],\n",
              "         [  198,  5962, 22307,    25,   198,  1639,   389,   477]]))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "    self.d_key = self.d_model // self.n_heads\n",
        "\n",
        "    self.wq = nn.Linear(d_model, d_model)\n",
        "    self.wk = nn.Linear(d_model, d_model)\n",
        "    self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.wo = nn.Linear(d_model, d_model)\n",
        "  def forward(self, ins, mask=None):\n",
        "    batch_size, seq_len, d_model = ins.size()\n",
        "    Q = self.wq(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "    K = self.wk(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "    V = self.wv(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "\n",
        "    scaled_dot_product = (Q @ K.transpose(2, 3)) / (self.d_model ** 0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "      scaled_dot_product += mask\n",
        "\n",
        "    attn_scores = F.softmax(scaled_dot_product, dim=-1) @ V\n",
        "    attn_scores = attn_scores.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "    return self.wo(attn_scores)\n",
        "MHA = MultiHeadAttention(32, 4)\n",
        "MHA(torch.randn(2, 16, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPtZtDsYakHh",
        "outputId": "1189fbd7-69db-4f14-fa6f-5758ee0390b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1876,  0.1363,  0.0678,  ..., -0.1028,  0.1414, -0.0776],\n",
              "         [-0.1812,  0.1432,  0.0522,  ..., -0.0997,  0.1453, -0.0724],\n",
              "         [-0.1832,  0.1258,  0.0397,  ..., -0.1007,  0.1347, -0.0795],\n",
              "         ...,\n",
              "         [-0.1698,  0.1284,  0.0637,  ..., -0.1138,  0.1245, -0.0730],\n",
              "         [-0.1845,  0.1589,  0.0566,  ..., -0.0982,  0.1473, -0.0760],\n",
              "         [-0.1824,  0.1383,  0.0624,  ..., -0.1017,  0.1360, -0.0719]],\n",
              "\n",
              "        [[-0.1146,  0.1173,  0.1476,  ..., -0.2827,  0.2060, -0.0723],\n",
              "         [-0.1079,  0.1302,  0.1606,  ..., -0.2923,  0.2371, -0.0788],\n",
              "         [-0.1305,  0.1058,  0.1719,  ..., -0.3135,  0.2192, -0.0797],\n",
              "         ...,\n",
              "         [-0.0948,  0.1529,  0.1157,  ..., -0.2403,  0.1726, -0.0373],\n",
              "         [-0.1082,  0.1400,  0.1395,  ..., -0.2814,  0.2133, -0.0204],\n",
              "         [-0.1185,  0.1393,  0.1558,  ..., -0.2868,  0.1977, -0.0504]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_size, hidden_size, out_size):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(in_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, out_size)\n",
        "    self.gelu = nn.GELU()\n",
        "  def forward(self, ins):\n",
        "    acts = self.gelu(self.l1(ins))\n",
        "    return self.l2(acts)"
      ],
      "metadata": {
        "id": "JcJ9yLT9hRXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_heads, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.MHA = MultiHeadAttention(d_model, n_heads)\n",
        "    self.MLP = MLP(d_model, 4*d_model, d_model)\n",
        "    self.layernorm1 = nn.LayerNorm(d_model)\n",
        "    self.layernorm2 = nn.LayerNorm(d_model)\n",
        "  def forward(self, ins, mask=None):\n",
        "    res1 = ins.clone()\n",
        "    attn_result = self.MHA(ins, mask=mask)\n",
        "    norm_result = self.layernorm1(attn_result)\n",
        "    norm_result += res1\n",
        "    res2 = norm_result.clone()\n",
        "    mlp_result = self.MLP(norm_result)\n",
        "    mlp_result_norm = self.layernorm2(mlp_result)\n",
        "    return mlp_result_norm + res2"
      ],
      "metadata": {
        "id": "pJ_EKzsag25J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, n_layers=2, n_heads=4, d_model=64):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.block_size = block_size\n",
        "    self.n_layers = n_layers\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.position_embedding = nn.Embedding(block_size, d_model)\n",
        "    self.decoder_stack = nn.ModuleList([\n",
        "        DecoderBlock(vocab_size, d_model, n_heads) for _ in range(n_layers)\n",
        "    ])\n",
        "    self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "    #self.output_proj.weight = self.token_embedding.weight\n",
        "  def forward(self, ins, targets=None):\n",
        "    B, T = ins.size()\n",
        "    x = self.token_embedding(ins)\n",
        "    input_indices = torch.arange(T)\n",
        "    x += self.position_embedding(input_indices)\n",
        "\n",
        "    look_ahead_mask = torch.triu(\n",
        "        torch.ones((T, T)), diagonal=1\n",
        "    )\n",
        "    look_ahead_mask.masked_fill_(look_ahead_mask == 1, float(\"-inf\"))\n",
        "    for decoder in self.decoder_stack:\n",
        "      x = decoder(x, mask=look_ahead_mask)\n",
        "    logits = self.output_proj(x)\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "    return logits, loss\n",
        "my_GPT = GPT(enc.n_vocab, 32, 12, 12, 768)"
      ],
      "metadata": {
        "id": "3dsbJ4B5aY19"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = dl.next()\n",
        "logits, loss = my_GPT(x, y)\n",
        "print(logits.shape, loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ6HIDlREtJy",
        "outputId": "c001bfcc-dfce-4447-b7b3-0c3ac5e47a29"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 50257]) 15.353865623474121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "block_size = 32\n",
        "n_layers = 4\n",
        "n_heads = 4\n",
        "d_model = 128\n",
        "lr = 3e-4\n",
        "\n",
        "my_GPT = GPT(enc.n_vocab, block_size, n_layers, n_heads, d_model)\n",
        "optim = torch.optim.AdamW(my_GPT.parameters(), lr=lr)\n",
        "data_loader = DataLoader(train_text, batch_size, block_size)\n",
        "\n",
        "log_interval = 50\n",
        "max_steps = 3000\n",
        "print(\"Steps per epoch:\", data_loader.steps_per_epoch())\n",
        "print(f\"GPT Parameters: {sum(p.numel() for p in my_GPT.parameters()) / 1e6} million\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MyAbAJw_DlZ",
        "outputId": "4fc14e55-56bf-4669-d44b-009932d52678"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps per epoch: 2500\n",
            "GPT Parameters: 13.713233 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"high\")"
      ],
      "metadata": {
        "id": "2U2yA8tmw5og"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "for step in range(max_steps):\n",
        "  step_start = time.time()\n",
        "  x, y = data_loader.next()\n",
        "  logits, loss = my_GPT(x, y)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print(f\"Step {step}, loss: {loss.item()}, time: {round((time.time() - step_start) * 1e3, 2)} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDzsArmPAR-B",
        "outputId": "9a3c2cec-114a-4cf5-c4fc-500a88bb8158"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, loss: 12.357880592346191, time: 405.67 ms\n",
            "Step 50, loss: 8.230766296386719, time: 304.09 ms\n",
            "Step 100, loss: 5.759158611297607, time: 243.55 ms\n",
            "Step 150, loss: 5.9091057777404785, time: 245.58 ms\n",
            "Step 200, loss: 4.242574691772461, time: 243.56 ms\n",
            "Step 250, loss: 5.715686321258545, time: 242.08 ms\n",
            "Step 300, loss: 4.924989223480225, time: 242.25 ms\n",
            "Step 350, loss: 6.233471393585205, time: 254.71 ms\n",
            "Step 400, loss: 7.218895435333252, time: 245.02 ms\n",
            "Step 450, loss: 5.946331024169922, time: 267.01 ms\n",
            "Step 500, loss: 6.201785564422607, time: 251.0 ms\n",
            "Step 550, loss: 5.575164318084717, time: 255.19 ms\n",
            "Step 600, loss: 6.235467910766602, time: 245.02 ms\n",
            "Step 650, loss: 6.295952796936035, time: 256.05 ms\n",
            "Step 700, loss: 6.352460861206055, time: 252.03 ms\n",
            "Step 750, loss: 6.290209770202637, time: 252.2 ms\n",
            "Step 800, loss: 4.736455917358398, time: 247.3 ms\n",
            "Step 850, loss: 6.201642990112305, time: 239.98 ms\n",
            "Step 900, loss: 4.767206192016602, time: 243.85 ms\n",
            "Step 950, loss: 5.344666004180908, time: 249.22 ms\n",
            "Step 1000, loss: 4.073705196380615, time: 247.62 ms\n",
            "Step 1050, loss: 5.847400188446045, time: 244.71 ms\n",
            "Step 1100, loss: 4.9564361572265625, time: 254.05 ms\n",
            "Step 1150, loss: 5.194848537445068, time: 248.63 ms\n",
            "Step 1200, loss: 5.4065260887146, time: 349.87 ms\n",
            "Step 1250, loss: 5.354774475097656, time: 369.51 ms\n",
            "Step 1300, loss: 6.418687343597412, time: 365.51 ms\n",
            "Step 1350, loss: 6.034974098205566, time: 364.21 ms\n",
            "Step 1400, loss: 5.199052333831787, time: 258.11 ms\n",
            "Step 1450, loss: 5.591334342956543, time: 245.25 ms\n",
            "Step 1500, loss: 4.1637725830078125, time: 248.23 ms\n",
            "Step 1550, loss: 4.909228801727295, time: 248.24 ms\n",
            "Step 1600, loss: 4.617430686950684, time: 252.74 ms\n",
            "Step 1650, loss: 6.9489850997924805, time: 256.33 ms\n",
            "Step 1700, loss: 5.319138050079346, time: 249.85 ms\n",
            "Step 1750, loss: 5.366188049316406, time: 245.73 ms\n",
            "Step 1800, loss: 5.453361988067627, time: 243.02 ms\n",
            "Step 1850, loss: 5.852756023406982, time: 245.0 ms\n",
            "Step 1900, loss: 5.544544219970703, time: 247.4 ms\n",
            "Step 1950, loss: 4.7448577880859375, time: 251.77 ms\n",
            "Step 2000, loss: 6.00775146484375, time: 251.77 ms\n",
            "Step 2050, loss: 4.4544677734375, time: 244.83 ms\n",
            "Step 2100, loss: 5.598586082458496, time: 254.11 ms\n",
            "Step 2150, loss: 5.0840582847595215, time: 248.92 ms\n",
            "Step 2200, loss: 4.54543924331665, time: 243.07 ms\n",
            "Step 2250, loss: 5.814681053161621, time: 251.61 ms\n",
            "Step 2300, loss: 5.334591388702393, time: 253.87 ms\n",
            "Step 2350, loss: 4.93135404586792, time: 371.04 ms\n",
            "Step 2400, loss: 3.1242570877075195, time: 349.82 ms\n",
            "Step 2450, loss: 4.492617130279541, time: 249.02 ms\n",
            "Step 2500, loss: 5.81558895111084, time: 248.34 ms\n",
            "Step 2550, loss: 4.592728614807129, time: 251.46 ms\n",
            "Step 2600, loss: 5.283541202545166, time: 264.1 ms\n",
            "Step 2650, loss: 3.5840282440185547, time: 256.07 ms\n",
            "Step 2700, loss: 4.121451377868652, time: 247.94 ms\n",
            "Step 2750, loss: 4.840407848358154, time: 250.17 ms\n",
            "Step 2800, loss: 4.92106819152832, time: 249.77 ms\n",
            "Step 2850, loss: 5.024357795715332, time: 248.33 ms\n",
            "Step 2900, loss: 4.187086582183838, time: 247.07 ms\n",
            "Step 2950, loss: 4.542717933654785, time: 248.33 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"First Citizen:\"\n",
        "input_tokens = enc.encode(prompt)\n",
        "output_tokens = enc.encode(prompt)\n",
        "for x in range(100):\n",
        "  if len(input_tokens) > block_size:\n",
        "    input_tokens = input_tokens[1:]\n",
        "  context_tensor = torch.tensor(input_tokens).view(1, -1)\n",
        "  logits, loss = my_GPT(context_tensor)\n",
        "  probs = F.softmax(logits[:, -1, :])\n",
        "  result = torch.multinomial(probs, num_samples=1).item()\n",
        "  input_tokens.append(result)\n",
        "  output_tokens.append(result)\n",
        "print(enc.decode(output_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn5PDMKwdvvF",
        "outputId": "a3e4b5ee-648f-45fb-d6d2-9216e81af36f"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-97-70fa2a20a10d>:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = F.softmax(logits[:, -1, :])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "I pray done a last: 'tis a man, the same.\n",
            "Because'd, we must go upon be see dead?\n",
            "\n",
            "Keep fares:\n",
            "You will slander a gods you by from perove?\n",
            "\n",
            "First longAT'Tis already, my poor father'st men.\n",
            "\n",
            "KING EDWARD IV:\n",
            "By comfort, look I had doth, my hearted gone,\n",
            "And being soon the best.\n",
            "\n",
            "Second sides:\n",
            "But been, as\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}