{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trickshotblaster/nn-practices/blob/main/TransformerV4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "a1c6112c-571b-41ad-9087-f061c6d1b377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt2\")\n",
        "print(enc.n_vocab)\n",
        "enc.decode(enc.encode(\"Hello world!\"))"
      ],
      "metadata": {
        "id": "lIZZax_FG4oj",
        "outputId": "81caa631-6523-4faf-ce35-592363b9d555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50257\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello world!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f: # input.txt\n",
        "    text = f.read()\n",
        "print(text[:100])"
      ],
      "metadata": {
        "id": "v4_640nZH6ze",
        "outputId": "db335365-a84d-400a-e43a-3d4e1f21ee52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-13 22:03:29--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-06-13 22:03:29 (17.7 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"codeparrot/github-code\", split=\"train[0.1%]\")\n",
        "\n",
        "print(ds)\n",
        "print(len(text))"
      ],
      "metadata": {
        "id": "-p7RBV93n7M3",
        "outputId": "05f37b5c-7f2f-43b2-b10f-e4a558d97a83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607,
          "referenced_widgets": [
            "8c32e34da95648c6ab347cf8aa4a2240",
            "0a648a6f76684a3cbe410a3591833646",
            "53a02ff16ec44567a44cc72fd8ccba98",
            "c9de2573c8ba4fceaebcce7e06d0a606",
            "9db638554e264c2daa32076e40c45cbf",
            "7d9fdfe51aed49d8834cae2046402cd1",
            "faffc9a8ddc0402e847bbb3da236b95b",
            "d6dc596db5f74c17a92d49dc6fdd0894",
            "234bbcf368f748f5bf0ab75007d91cbf",
            "e794948a65e546888f9fa54236e12cc5",
            "79bb605a13c648b3b55dfcacd56eb062"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0/1126 [00:00<?, ?files/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c32e34da95648c6ab347cf8aa4a2240"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_amount = 0.95\n",
        "idx = int(train_amount * len(text))\n",
        "train_text = text[:idx]\n",
        "val_text = text[idx:]"
      ],
      "metadata": {
        "id": "HG6tD8arIGqu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "xFvovfpvaNg_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Z55xQhpY21KG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class DataLoader:\n",
        "  def __init__(self, text, batch_size, block_size, random_sample=True):\n",
        "    self.text = torch.tensor(enc.encode(text)).to(device)\n",
        "    self.batch_size = batch_size\n",
        "    self.block_size = block_size\n",
        "    self.current_pos = 0\n",
        "    self.random_sample = random_sample\n",
        "  def steps_per_epoch(self):\n",
        "    return len(self.text) // (self.batch_size * self.block_size)\n",
        "  def next(self):\n",
        "    if self.current_pos + self.batch_size * self.block_size + 1 >= len(self.text):\n",
        "      self.current_pos = 0\n",
        "    if self.random_sample:\n",
        "      idx = int((random.random() * len(self.text)) - (self.batch_size * self.block_size + 1) - 1)\n",
        "      buf = self.text[idx:idx + (self.batch_size * self.block_size + 1)] #[self.current_pos:self.current_pos + self.batch_size * self.block_size + 1]\n",
        "      if len(buf) == 0:\n",
        "        return self.next()\n",
        "    else:\n",
        "      buf = self.text[self.current_pos:self.current_pos + self.batch_size * self.block_size + 1]\n",
        "    ins = buf[:-1].view(self.batch_size, self.block_size)\n",
        "    tgts = buf[1:].view(self.batch_size, self.block_size)\n",
        "    self.current_pos += self.batch_size * self.block_size + 1\n",
        "    return ins, tgts\n",
        "dl = DataLoader(train_text, 4, 8)\n",
        "dl.next()"
      ],
      "metadata": {
        "id": "pJN-mDlGIW_N",
        "outputId": "fc7f44c5-6a43-4c02-ba55-412263338575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 7500,   592,    11,   198,  2504,   804,  1165, 37248],\n",
              "         [  287,   674,  2219, 14298,    25,   198,  3237,  1276],\n",
              "         [  307,   772,   287,   674,  1230,    13,   198,  1639],\n",
              "         [ 4145,  1873,  1549,    11,   314,   481,   467,  6808]]),\n",
              " tensor([[  592,    11,   198,  2504,   804,  1165, 37248,   287],\n",
              "         [  674,  2219, 14298,    25,   198,  3237,  1276,   307],\n",
              "         [  772,   287,   674,  1230,    13,   198,  1639,  4145],\n",
              "         [ 1873,  1549,    11,   314,   481,   467,  6808,  1497]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "    self.d_key = self.d_model // self.n_heads\n",
        "\n",
        "    self.wq = nn.Linear(d_model, d_model)\n",
        "    self.wk = nn.Linear(d_model, d_model)\n",
        "    self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.wo = nn.Linear(d_model, d_model)\n",
        "  def forward(self, ins, mask=None):\n",
        "    batch_size, seq_len, d_model = ins.size()\n",
        "    Q = self.wq(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "    K = self.wk(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "    V = self.wv(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "\n",
        "    #scaled_dot_product = (Q @ K.transpose(2, 3)) / (self.d_model ** 0.5)\n",
        "\n",
        "    #if mask is not None:\n",
        "      #scaled_dot_product += mask\n",
        "\n",
        "    attn_scores = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)\n",
        "    #F.softmax(scaled_dot_product, dim=-1) @ V\n",
        "    attn_scores = attn_scores.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "    return self.wo(attn_scores)\n",
        "MHA = MultiHeadAttention(32, 4)\n",
        "MHA(torch.randn(2, 16, 32))"
      ],
      "metadata": {
        "id": "KPtZtDsYakHh",
        "outputId": "edee17ff-47f2-404d-ae67-707852c14a92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1183,  0.1336,  0.0290,  ..., -0.2397,  0.1399, -0.1039],\n",
              "         [ 0.1351,  0.1101,  0.0016,  ..., -0.2307,  0.1385, -0.1189],\n",
              "         [ 0.1755,  0.1172,  0.0071,  ..., -0.1999,  0.1167, -0.1345],\n",
              "         ...,\n",
              "         [ 0.1378,  0.1237,  0.0297,  ..., -0.1953,  0.1114, -0.1279],\n",
              "         [ 0.2406,  0.1049,  0.0135,  ..., -0.2194,  0.1763, -0.1585],\n",
              "         [ 0.0993,  0.1412,  0.0307,  ..., -0.2116,  0.1037, -0.1321]],\n",
              "\n",
              "        [[ 0.1755,  0.1363,  0.1488,  ..., -0.0733,  0.1255, -0.2467],\n",
              "         [ 0.1497,  0.1942,  0.1458,  ..., -0.1603,  0.0484, -0.1838],\n",
              "         [ 0.1338,  0.2105,  0.1820,  ..., -0.1377,  0.0490, -0.1816],\n",
              "         ...,\n",
              "         [ 0.1532,  0.1529,  0.1851,  ..., -0.1071,  0.0459, -0.1696],\n",
              "         [ 0.1916,  0.1314,  0.0623,  ..., -0.1553,  0.0754, -0.2610],\n",
              "         [ 0.1765,  0.1128,  0.1206,  ..., -0.1518,  0.0474, -0.1735]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_size, hidden_size, out_size):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(in_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, out_size)\n",
        "    self.gelu = nn.GELU()\n",
        "  def forward(self, ins):\n",
        "    acts = self.gelu(self.l1(ins))\n",
        "    return self.l2(acts)"
      ],
      "metadata": {
        "id": "JcJ9yLT9hRXb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_heads, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.MHA = MultiHeadAttention(d_model, n_heads)\n",
        "    self.MLP = MLP(d_model, 4*d_model, d_model)\n",
        "    self.layernorm1 = nn.LayerNorm(d_model)\n",
        "    self.layernorm2 = nn.LayerNorm(d_model)\n",
        "  def forward(self, ins, mask=None):\n",
        "    res1 = ins.clone()\n",
        "    attn_result = self.MHA(ins, mask=mask)\n",
        "    norm_result = self.layernorm1(attn_result)\n",
        "    norm_result += res1\n",
        "    res2 = norm_result.clone()\n",
        "    mlp_result = self.MLP(norm_result)\n",
        "    mlp_result_norm = self.layernorm2(mlp_result)\n",
        "    return mlp_result_norm + res2"
      ],
      "metadata": {
        "id": "pJ_EKzsag25J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, n_layers=2, n_heads=4, d_model=64):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.block_size = block_size\n",
        "    self.n_layers = n_layers\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.position_embedding = nn.Embedding(block_size, d_model)\n",
        "    self.decoder_stack = nn.ModuleList([\n",
        "        DecoderBlock(vocab_size, d_model, n_heads) for _ in range(n_layers)\n",
        "    ])\n",
        "    self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "    #self.output_proj.weight = self.token_embedding.weight\n",
        "  def forward(self, ins, targets=None):\n",
        "    B, T = ins.size()\n",
        "\n",
        "    x = self.token_embedding(ins.to(device))\n",
        "    input_indices = torch.arange(T).to(device)\n",
        "    x += self.position_embedding(input_indices)\n",
        "\n",
        "    look_ahead_mask = torch.triu(\n",
        "        torch.ones((T, T)), diagonal=1\n",
        "    )\n",
        "    look_ahead_mask.masked_fill_(look_ahead_mask == 1, float(\"-inf\"))\n",
        "    look_ahead_mask = look_ahead_mask.to(device)\n",
        "\n",
        "    for decoder in self.decoder_stack:\n",
        "      x = decoder(x, mask=look_ahead_mask)\n",
        "    logits = self.output_proj(x)\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      targets = targets.to(device)\n",
        "      loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "    return logits, loss\n",
        "my_GPT = GPT(enc.n_vocab, 32, 12, 12, 768).to(device)"
      ],
      "metadata": {
        "id": "3dsbJ4B5aY19"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = dl.next()\n",
        "logits, loss = my_GPT(x, y)\n",
        "print(logits.shape, loss.item())"
      ],
      "metadata": {
        "id": "YJ6HIDlREtJy",
        "outputId": "ee3a06c4-f2a1-4c23-933a-545325e049e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 50257]) 13.431506156921387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "block_size = 16\n",
        "n_layers = 2\n",
        "n_heads = 4\n",
        "d_model = 32\n",
        "lr = 3e-4\n",
        "\n",
        "\n",
        "\n",
        "my_GPT = GPT(enc.n_vocab, block_size, n_layers, n_heads, d_model)\n",
        "my_GPT = my_GPT.to(device)\n",
        "\n",
        "compile = True\n",
        "if compile and torch.cuda.is_available():\n",
        "  my_GPT = torch.compile(my_GPT)\n",
        "\n",
        "optim = torch.optim.AdamW(my_GPT.parameters(), lr=lr)\n",
        "data_loader = DataLoader(train_text, batch_size, block_size, random_sample=True)\n",
        "\n",
        "val_data_loader = DataLoader(val_text, batch_size, block_size, random_sample=False)\n",
        "val_interval = 200\n",
        "\n",
        "log_interval = 50\n",
        "max_steps = 3000\n",
        "print(\"Steps per epoch:\", data_loader.steps_per_epoch())\n",
        "print(f\"GPT Parameters: {sum(p.numel() for p in my_GPT.parameters()) / 1e6} million\")"
      ],
      "metadata": {
        "id": "7MyAbAJw_DlZ",
        "outputId": "a993f5d5-f609-46f7-dab1-8770c4ae2e7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps per epoch: 5000\n",
            "GPT Parameters: 3.292625 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"high\")"
      ],
      "metadata": {
        "id": "2U2yA8tmw5og"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "best_val_loss = float(\"inf\")\n",
        "my_GPT.train()\n",
        "for step in range(max_steps + 1):\n",
        "  step_start = time.time()\n",
        "  x, y = data_loader.next()\n",
        "  logits, loss = my_GPT(x, y)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print(f\"Step {step}, loss: {loss.item()}, time: {round((time.time() - step_start) * 1e3, 2)} ms\")\n",
        "  if step % val_interval == 0:\n",
        "    with torch.no_grad():\n",
        "      val_loss = 0\n",
        "      for val_step in range(val_data_loader.steps_per_epoch()):\n",
        "        val_x, val_y = val_data_loader.next()\n",
        "        logits, loss = my_GPT(val_x, val_y)\n",
        "        val_loss += loss\n",
        "      val_loss /= val_data_loader.steps_per_epoch()\n",
        "      if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(my_GPT.state_dict(), 'best_model.pth')\n",
        "      print(f\"Val loss for step {step}: {val_loss}\")\n",
        "my_GPT.load_state_dict(torch.load('best_model.pth'))\n",
        "my_GPT.eval()"
      ],
      "metadata": {
        "id": "tDzsArmPAR-B",
        "outputId": "7256d112-2f06-4676-c2d8-c1a3d56432cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, loss: 11.783772468566895, time: 113.32 ms\n",
            "Val loss for step 0: 11.50425910949707\n",
            "Step 50, loss: 10.398383140563965, time: 67.84 ms\n",
            "Step 100, loss: 8.120105743408203, time: 113.62 ms\n",
            "Step 150, loss: 7.678050518035889, time: 66.83 ms\n",
            "Step 200, loss: 7.893157005310059, time: 66.8 ms\n",
            "Val loss for step 200: 7.408690452575684\n",
            "Step 250, loss: 7.235362529754639, time: 87.85 ms\n",
            "Step 300, loss: 6.955040454864502, time: 87.27 ms\n",
            "Step 350, loss: 6.1286396980285645, time: 112.71 ms\n",
            "Step 400, loss: 6.125448703765869, time: 83.69 ms\n",
            "Val loss for step 400: 6.776885986328125\n",
            "Step 450, loss: 7.455333232879639, time: 112.11 ms\n",
            "Step 500, loss: 6.582122325897217, time: 78.17 ms\n",
            "Step 550, loss: 6.479583263397217, time: 84.92 ms\n",
            "Step 600, loss: 7.278920650482178, time: 92.71 ms\n",
            "Val loss for step 600: 6.578640937805176\n",
            "Step 650, loss: 6.18936014175415, time: 84.25 ms\n",
            "Step 700, loss: 5.844069957733154, time: 93.15 ms\n",
            "Step 750, loss: 6.245770454406738, time: 80.64 ms\n",
            "Step 800, loss: 7.812077045440674, time: 84.53 ms\n",
            "Val loss for step 800: 6.472287654876709\n",
            "Step 850, loss: 6.020776748657227, time: 91.5 ms\n",
            "Step 900, loss: 5.8102264404296875, time: 68.02 ms\n",
            "Step 950, loss: 6.292858123779297, time: 95.42 ms\n",
            "Step 1000, loss: 5.710976600646973, time: 86.98 ms\n",
            "Val loss for step 1000: 6.371785640716553\n",
            "Step 1050, loss: 5.666871070861816, time: 127.31 ms\n",
            "Step 1100, loss: 5.575780868530273, time: 92.72 ms\n",
            "Step 1150, loss: 6.243680477142334, time: 84.57 ms\n",
            "Step 1200, loss: 6.719781875610352, time: 129.5 ms\n",
            "Val loss for step 1200: 6.321181774139404\n",
            "Step 1250, loss: 6.91360330581665, time: 68.16 ms\n",
            "Step 1300, loss: 6.206282138824463, time: 76.03 ms\n",
            "Step 1350, loss: 6.334336280822754, time: 66.02 ms\n",
            "Step 1400, loss: 5.424272060394287, time: 86.22 ms\n",
            "Val loss for step 1400: 6.241862773895264\n",
            "Step 1450, loss: 5.933999538421631, time: 68.86 ms\n",
            "Step 1500, loss: 6.074019908905029, time: 68.31 ms\n",
            "Step 1550, loss: 6.130147933959961, time: 94.69 ms\n",
            "Step 1600, loss: 6.485631465911865, time: 69.83 ms\n",
            "Val loss for step 1600: 6.206302642822266\n",
            "Step 1650, loss: 6.251200199127197, time: 84.66 ms\n",
            "Step 1700, loss: 5.8256354331970215, time: 67.14 ms\n",
            "Step 1750, loss: 4.720343112945557, time: 68.88 ms\n",
            "Step 1800, loss: 8.019330978393555, time: 115.47 ms\n",
            "Val loss for step 1800: 6.141988277435303\n",
            "Step 1850, loss: 7.280463218688965, time: 67.95 ms\n",
            "Step 1900, loss: 6.334874629974365, time: 85.47 ms\n",
            "Step 1950, loss: 7.086774826049805, time: 84.11 ms\n",
            "Step 2000, loss: 5.365054130554199, time: 84.29 ms\n",
            "Val loss for step 2000: 6.10237455368042\n",
            "Step 2050, loss: 7.041494369506836, time: 67.97 ms\n",
            "Step 2100, loss: 6.113912582397461, time: 93.49 ms\n",
            "Step 2150, loss: 5.363224983215332, time: 84.29 ms\n",
            "Step 2200, loss: 5.7722978591918945, time: 85.96 ms\n",
            "Val loss for step 2200: 6.066169261932373\n",
            "Step 2250, loss: 5.85915470123291, time: 83.83 ms\n",
            "Step 2300, loss: 5.462168216705322, time: 70.73 ms\n",
            "Step 2350, loss: 5.645531177520752, time: 72.3 ms\n",
            "Step 2400, loss: 6.002951622009277, time: 69.98 ms\n",
            "Val loss for step 2400: 6.045459747314453\n",
            "Step 2450, loss: 6.239501953125, time: 67.88 ms\n",
            "Step 2500, loss: 6.323644161224365, time: 66.68 ms\n",
            "Step 2550, loss: 5.3984479904174805, time: 74.35 ms\n",
            "Step 2600, loss: 6.509132385253906, time: 75.81 ms\n",
            "Val loss for step 2600: 5.998952865600586\n",
            "Step 2650, loss: 5.248809337615967, time: 70.82 ms\n",
            "Step 2700, loss: 5.710243225097656, time: 67.24 ms\n",
            "Step 2750, loss: 5.273414611816406, time: 86.23 ms\n",
            "Step 2800, loss: 5.794351100921631, time: 75.53 ms\n",
            "Val loss for step 2800: 5.962948322296143\n",
            "Step 2850, loss: 5.248717308044434, time: 86.55 ms\n",
            "Step 2900, loss: 5.001456260681152, time: 71.44 ms\n",
            "Step 2950, loss: 4.829115390777588, time: 107.61 ms\n",
            "Step 3000, loss: 5.791368007659912, time: 89.46 ms\n",
            "Val loss for step 3000: 5.92037296295166\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (token_embedding): Embedding(50257, 32)\n",
              "  (position_embedding): Embedding(16, 32)\n",
              "  (decoder_stack): ModuleList(\n",
              "    (0-1): 2 x DecoderBlock(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (MHA): MultiHeadAttention(\n",
              "        (wq): Linear(in_features=32, out_features=32, bias=True)\n",
              "        (wk): Linear(in_features=32, out_features=32, bias=True)\n",
              "        (wv): Linear(in_features=32, out_features=32, bias=True)\n",
              "        (wo): Linear(in_features=32, out_features=32, bias=True)\n",
              "      )\n",
              "      (MLP): MLP(\n",
              "        (l1): Linear(in_features=32, out_features=128, bias=True)\n",
              "        (l2): Linear(in_features=128, out_features=32, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "      )\n",
              "      (layernorm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "      (layernorm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (output_proj): Linear(in_features=32, out_features=50257, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"ROMEO:\"\n",
        "input_tokens = enc.encode(prompt)\n",
        "output_tokens = enc.encode(prompt)\n",
        "for x in range(200):\n",
        "  if len(input_tokens) > block_size:\n",
        "    input_tokens = input_tokens[1:]\n",
        "  context_tensor = torch.tensor(input_tokens).view(1, -1).to(device)\n",
        "\n",
        "  logits, loss = my_GPT(context_tensor)\n",
        "  probs = F.softmax(logits[:, -1, :])\n",
        "  result = torch.multinomial(probs, num_samples=1).item()\n",
        "  input_tokens.append(result)\n",
        "  output_tokens.append(result)\n",
        "print(enc.decode(output_tokens))"
      ],
      "metadata": {
        "id": "Xn5PDMKwdvvF",
        "outputId": "224b3e3a-a968-4a64-ee1b-f06ee129b933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-1494ae2c439c>:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = F.softmax(logits[:, -1, :])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "INC John me.\n",
            "\n",
            "IUS:\n",
            "Some goes speak,\n",
            "I advertise to progress she hear all: I al by the tale the empty lodge\n",
            "\n",
            "alledINAAR:\n",
            "Sh for me, thenst.\n",
            "That orderly as inKE it heirEN:\n",
            "He, sir the thee:\n",
            "Should, forthself; where here her you d I will had '\n",
            "To she, his nuns. flesh:\n",
            "\n",
            "No's face menW weep makes of was at not in:\n",
            "What of whyCome not act Paul me to of new by counsel example that, then\n",
            "Drawhee deceit something warriorsourO:\n",
            "Art you?\n",
            "HaveER be i in fun instantly, 'el:\n",
            "Have hisuke she hath we, dependent will\n",
            "To bount oddsow,\n",
            "Theity his loved him VI upon a, sir,tisbuck,\n",
            "To else to burning loss this dowQueen being lips, his hanging your.\n",
            "\n",
            "D bending an who:\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c32e34da95648c6ab347cf8aa4a2240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a648a6f76684a3cbe410a3591833646",
              "IPY_MODEL_53a02ff16ec44567a44cc72fd8ccba98",
              "IPY_MODEL_c9de2573c8ba4fceaebcce7e06d0a606"
            ],
            "layout": "IPY_MODEL_9db638554e264c2daa32076e40c45cbf"
          }
        },
        "0a648a6f76684a3cbe410a3591833646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d9fdfe51aed49d8834cae2046402cd1",
            "placeholder": "​",
            "style": "IPY_MODEL_faffc9a8ddc0402e847bbb3da236b95b",
            "value": "Downloading data:   4%"
          }
        },
        "53a02ff16ec44567a44cc72fd8ccba98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6dc596db5f74c17a92d49dc6fdd0894",
            "max": 1126,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_234bbcf368f748f5bf0ab75007d91cbf",
            "value": 44
          }
        },
        "c9de2573c8ba4fceaebcce7e06d0a606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e794948a65e546888f9fa54236e12cc5",
            "placeholder": "​",
            "style": "IPY_MODEL_79bb605a13c648b3b55dfcacd56eb062",
            "value": " 44/1126 [07:13&lt;54:20,  3.01s/files]"
          }
        },
        "9db638554e264c2daa32076e40c45cbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d9fdfe51aed49d8834cae2046402cd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faffc9a8ddc0402e847bbb3da236b95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6dc596db5f74c17a92d49dc6fdd0894": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "234bbcf368f748f5bf0ab75007d91cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e794948a65e546888f9fa54236e12cc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79bb605a13c648b3b55dfcacd56eb062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}