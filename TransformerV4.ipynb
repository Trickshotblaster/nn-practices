{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trickshotblaster/nn-practices/blob/main/TransformerV4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "f62af0f6-2c1f-4bc4-acca-bd3210802ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt2\")\n",
        "print(enc.n_vocab)\n",
        "enc.decode(enc.encode(\"Hello world!\"))"
      ],
      "metadata": {
        "id": "lIZZax_FG4oj",
        "outputId": "6106b370-20dd-425a-98c7-aa8126cf52a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50257\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello world!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f: # input.txt\n",
        "    text = f.read()\n",
        "print(text[:100])"
      ],
      "metadata": {
        "id": "v4_640nZH6ze",
        "outputId": "647c9267-0e81-4d20-87ad-43cef6801257",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-13 18:59:00--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "input.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-06-13 18:59:00 (22.4 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_amount = 0.95\n",
        "idx = int(train_amount * len(text))\n",
        "train_text = text[:idx]\n",
        "val_text = text[idx:]"
      ],
      "metadata": {
        "id": "HG6tD8arIGqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "xFvovfpvaNg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Z55xQhpY21KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class DataLoader:\n",
        "  def __init__(self, text, batch_size, block_size, random_sample=True):\n",
        "    self.text = torch.tensor(enc.encode(text)).to(device)\n",
        "    self.batch_size = batch_size\n",
        "    self.block_size = block_size\n",
        "    self.current_pos = 0\n",
        "    self.random_sample = random_sample\n",
        "  def steps_per_epoch(self):\n",
        "    return len(self.text) // (self.batch_size * self.block_size)\n",
        "  def next(self):\n",
        "    if self.current_pos + self.batch_size * self.block_size + 1 >= len(self.text):\n",
        "      self.current_pos = 0\n",
        "    if self.random_sample:\n",
        "      idx = int((random.random() * len(self.text)) - (self.batch_size * self.block_size + 1) - 1)\n",
        "      buf = self.text[idx:idx + (self.batch_size * self.block_size + 1)] #[self.current_pos:self.current_pos + self.batch_size * self.block_size + 1]\n",
        "      if len(buf) == 0:\n",
        "        return self.next()\n",
        "    else:\n",
        "      buf = self.text[self.current_pos:self.current_pos + self.batch_size * self.block_size + 1]\n",
        "    ins = buf[:-1].view(self.batch_size, self.block_size)\n",
        "    tgts = buf[1:].view(self.batch_size, self.block_size)\n",
        "    self.current_pos += self.batch_size * self.block_size + 1\n",
        "    return ins, tgts\n",
        "dl = DataLoader(train_text, 4, 8)\n",
        "dl.next()"
      ],
      "metadata": {
        "id": "pJN-mDlGIW_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "    self.d_key = self.d_model // self.n_heads\n",
        "\n",
        "    self.wq = nn.Linear(d_model, d_model)\n",
        "    self.wk = nn.Linear(d_model, d_model)\n",
        "    self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.wo = nn.Linear(d_model, d_model)\n",
        "  def forward(self, ins, mask=None):\n",
        "    batch_size, seq_len, d_model = ins.size()\n",
        "    Q = self.wq(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "    K = self.wk(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "    V = self.wv(ins).view(batch_size, seq_len, self.n_heads, self.d_key).transpose(1, 2)\n",
        "\n",
        "    #scaled_dot_product = (Q @ K.transpose(2, 3)) / (self.d_model ** 0.5)\n",
        "\n",
        "    #if mask is not None:\n",
        "      #scaled_dot_product += mask\n",
        "\n",
        "    attn_scores = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)\n",
        "    #F.softmax(scaled_dot_product, dim=-1) @ V\n",
        "    attn_scores = attn_scores.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "    return self.wo(attn_scores)\n",
        "MHA = MultiHeadAttention(32, 4)\n",
        "MHA(torch.randn(2, 16, 32))"
      ],
      "metadata": {
        "id": "KPtZtDsYakHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_size, hidden_size, out_size):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(in_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, out_size)\n",
        "    self.gelu = nn.GELU()\n",
        "  def forward(self, ins):\n",
        "    acts = self.gelu(self.l1(ins))\n",
        "    return self.l2(acts)"
      ],
      "metadata": {
        "id": "JcJ9yLT9hRXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_heads, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.MHA = MultiHeadAttention(d_model, n_heads)\n",
        "    self.MLP = MLP(d_model, 4*d_model, d_model)\n",
        "    self.layernorm1 = nn.LayerNorm(d_model)\n",
        "    self.layernorm2 = nn.LayerNorm(d_model)\n",
        "  def forward(self, ins, mask=None):\n",
        "    res1 = ins.clone()\n",
        "    attn_result = self.MHA(ins, mask=mask)\n",
        "    norm_result = self.layernorm1(attn_result)\n",
        "    norm_result += res1\n",
        "    res2 = norm_result.clone()\n",
        "    mlp_result = self.MLP(norm_result)\n",
        "    mlp_result_norm = self.layernorm2(mlp_result)\n",
        "    return mlp_result_norm + res2"
      ],
      "metadata": {
        "id": "pJ_EKzsag25J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, n_layers=2, n_heads=4, d_model=64):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.block_size = block_size\n",
        "    self.n_layers = n_layers\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.position_embedding = nn.Embedding(block_size, d_model)\n",
        "    self.decoder_stack = nn.ModuleList([\n",
        "        DecoderBlock(vocab_size, d_model, n_heads) for _ in range(n_layers)\n",
        "    ])\n",
        "    self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "    #self.output_proj.weight = self.token_embedding.weight\n",
        "  def forward(self, ins, targets=None):\n",
        "    B, T = ins.size()\n",
        "\n",
        "    x = self.token_embedding(ins.to(device))\n",
        "    input_indices = torch.arange(T).to(device)\n",
        "    x += self.position_embedding(input_indices)\n",
        "\n",
        "    look_ahead_mask = torch.triu(\n",
        "        torch.ones((T, T)), diagonal=1\n",
        "    )\n",
        "    look_ahead_mask.masked_fill_(look_ahead_mask == 1, float(\"-inf\"))\n",
        "    look_ahead_mask = look_ahead_mask.to(device)\n",
        "\n",
        "    for decoder in self.decoder_stack:\n",
        "      x = decoder(x, mask=look_ahead_mask)\n",
        "    logits = self.output_proj(x)\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      targets = targets.to(device)\n",
        "      loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "    return logits, loss\n",
        "my_GPT = GPT(enc.n_vocab, 32, 12, 12, 768).to(device)"
      ],
      "metadata": {
        "id": "3dsbJ4B5aY19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = dl.next()\n",
        "logits, loss = my_GPT(x, y)\n",
        "print(logits.shape, loss.item())"
      ],
      "metadata": {
        "id": "YJ6HIDlREtJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "block_size = 32\n",
        "n_layers = 4\n",
        "n_heads = 4\n",
        "d_model = 128\n",
        "lr = 3e-4\n",
        "\n",
        "\n",
        "\n",
        "my_GPT = GPT(enc.n_vocab, block_size, n_layers, n_heads, d_model)\n",
        "my_GPT = my_GPT.to(device)\n",
        "\n",
        "compile = True\n",
        "if compile and torch.cuda.is_available():\n",
        "  my_GPT = torch.compile(my_GPT)\n",
        "\n",
        "optim = torch.optim.AdamW(my_GPT.parameters(), lr=lr)\n",
        "data_loader = DataLoader(train_text, batch_size, block_size, random_sample=True)\n",
        "\n",
        "val_data_loader = DataLoader(val_text, batch_size, block_size, random_sample=False)\n",
        "val_interval = 200\n",
        "\n",
        "log_interval = 50\n",
        "max_steps = 3000\n",
        "print(\"Steps per epoch:\", data_loader.steps_per_epoch())\n",
        "print(f\"GPT Parameters: {sum(p.numel() for p in my_GPT.parameters()) / 1e6} million\")"
      ],
      "metadata": {
        "id": "7MyAbAJw_DlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"high\")"
      ],
      "metadata": {
        "id": "2U2yA8tmw5og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "best_val_loss = float(\"inf\")\n",
        "my_GPT.train()\n",
        "for step in range(max_steps + 1):\n",
        "  step_start = time.time()\n",
        "  x, y = data_loader.next()\n",
        "  logits, loss = my_GPT(x, y)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print(f\"Step {step}, loss: {loss.item()}, time: {round((time.time() - step_start) * 1e3, 2)} ms\")\n",
        "  if step % val_interval == 0:\n",
        "    with torch.no_grad():\n",
        "      val_loss = 0\n",
        "      for val_step in range(val_data_loader.steps_per_epoch()):\n",
        "        val_x, val_y = val_data_loader.next()\n",
        "        logits, loss = my_GPT(val_x, val_y)\n",
        "        val_loss += loss\n",
        "      val_loss /= val_data_loader.steps_per_epoch()\n",
        "      if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(my_GPT.state_dict(), 'best_model.pth')\n",
        "      print(f\"Val loss for step {step}: {val_loss}\")\n",
        "my_GPT.load_state_dict(torch.load('best_model.pth'))\n",
        "my_GPT.eval()"
      ],
      "metadata": {
        "id": "tDzsArmPAR-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"ROMEO:\"\n",
        "input_tokens = enc.encode(prompt)\n",
        "output_tokens = enc.encode(prompt)\n",
        "for x in range(200):\n",
        "  if len(input_tokens) > block_size:\n",
        "    input_tokens = input_tokens[1:]\n",
        "  context_tensor = torch.tensor(input_tokens).view(1, -1).to(device)\n",
        "\n",
        "  logits, loss = my_GPT(context_tensor)\n",
        "  probs = F.softmax(logits[:, -1, :])\n",
        "  result = torch.multinomial(probs, num_samples=1).item()\n",
        "  input_tokens.append(result)\n",
        "  output_tokens.append(result)\n",
        "print(enc.decode(output_tokens))"
      ],
      "metadata": {
        "id": "Xn5PDMKwdvvF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}