{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMT5ir7n40Oo0HGN+s4UiUH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trickshotblaster/nn-practices/blob/main/TransformerV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mF1Wi04gwqoL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "raw_text = requests.get(url).text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_amount = 0.9\n",
        "train_text, val_text = raw_text[:int(len(raw_text) * train_amount)], raw_text[int(len(raw_text) * train_amount):]\n",
        "train_text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ARWYIXl9xFNT",
        "outputId": "a8a0785a-7173-4222-cdbb-e86aa8091005"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(raw_text)\n",
        "vocab.add(\"<|UNK|>\")\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "stoi = {char:i for i, char in enumerate(vocab)}\n",
        "itos = {i:char for i, char in enumerate(stoi)}\n",
        "itos[stoi['<|UNK|>']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YY3cBAtMxaNW",
        "outputId": "7596c136-903a-4b65-fca8-06f15ffe4410"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|UNK|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, stoi, itos):\n",
        "    self.stoi = stoi\n",
        "    self.itos = itos\n",
        "  def __len__(self):\n",
        "    return len(self.stoi) - 1\n",
        "  def encode(self, text):\n",
        "    return [stoi.get(char, stoi['<|UNK|>']) for char in text]\n",
        "  def decode(self, tokens):\n",
        "    return \"\".join([itos.get(token, '<|UNK|>') for token in tokens])\n",
        "tokenizer = Tokenizer(stoi, itos)\n",
        "tokenizer.decode(tokenizer.encode(\"hello world ^}|%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2tqyzMtdyA44",
        "outputId": "a149a682-68da-4c02-f443-ed1f1497fe4d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world <|UNK|><|UNK|><|UNK|><|UNK|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "GKc4Oey01Gif"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 16\n",
        "emb_dim = 32\n",
        "import math\n",
        "def create_positional_encoding_matrix(max_seq_len, emb_dim):\n",
        "  mask = torch.zeros((max_seq_len, emb_dim))\n",
        "  for pos in range(max_seq_len):\n",
        "    for i in range(0, emb_dim, 2):\n",
        "      mask[pos, i] = math.sin(pos/(10000 ** ((2*i)/emb_dim)))\n",
        "      mask[pos, i+1] = math.cos(pos/(10000 ** ((2*(i+1))/emb_dim)))\n",
        "  return mask\n",
        "positional_encoding_matrix = create_positional_encoding_matrix(max_seq_len, emb_dim)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(positional_encoding_matrix.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "rhLazuVi0Xs3",
        "outputId": "97134cd9-5e2d-4d60-f06e-f3f2635b4b92"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7aac6b491780>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedMultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_heads, emb_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "    assert emb_dim % n_heads == 0, \"Embedding dimension must be divisible by head number\"\n",
        "    self.n_heads = n_heads\n",
        "    self.emb_dim = emb_dim\n",
        "    self.head_size = emb_dim // n_heads\n",
        "    self.wq = nn.Linear(emb_dim, emb_dim)\n",
        "    self.wk = nn.Linear(emb_dim, emb_dim)\n",
        "    self.wv = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    self.wo = nn.Linear(emb_dim, emb_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len, emb_dim = x.size()\n",
        "    Q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_size).transpose(1, 2)\n",
        "    K = self.wk(x).view(batch_size, seq_len, self.n_heads, self.head_size).transpose(1, 2)\n",
        "    V = self.wv(x).view(batch_size, seq_len, self.n_heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "    scaled_dot_product = (torch.matmul(Q, K.transpose(2, 3)))/(self.head_size**0.5)\n",
        "\n",
        "    attn_mask = torch.triu(\n",
        "        torch.ones(seq_len, seq_len),\n",
        "        diagonal=1\n",
        "    ).to(device)\n",
        "    attn_mask.masked_fill_(attn_mask==1, float('-inf'))\n",
        "\n",
        "    scaled_dot_product += attn_mask\n",
        "\n",
        "    attn_scores = torch.matmul(F.softmax(scaled_dot_product, dim=-1), V)\n",
        "    attn_scores = attn_scores.view(batch_size, -1, self.n_heads * self.head_size)\n",
        "    attn_scores = self.dropout(attn_scores)\n",
        "    out = self.wo(attn_scores)\n",
        "\n",
        "    return out # (batch_size, seq_len, emb_dim)\n"
      ],
      "metadata": {
        "id": "t-Mcx23B5lFO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_size, hidden_size, out_size):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(in_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, out_size)\n",
        "    self.act = nn.GELU()\n",
        "  def forward(self, ins):\n",
        "    hidden = self.act(self.l1(ins))\n",
        "    return self.act(self.l2(hidden))"
      ],
      "metadata": {
        "id": "hW5QG80_K_2K"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, n_heads, emb_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.layernorm = nn.LayerNorm(emb_dim)\n",
        "    self.n_heads = n_heads\n",
        "    self.emb_dim = emb_dim\n",
        "    self.MHA = MaskedMultiHeadAttention(n_heads, emb_dim, dropout=dropout)\n",
        "    self.post_attention_dropout = nn.Dropout(dropout)\n",
        "    self.MLP = MLP(emb_dim, emb_dim * 4, emb_dim)\n",
        "    self.post_MLP_dropout = nn.Dropout(dropout)\n",
        "  def forward(self, ins):\n",
        "    residual = ins.clone()\n",
        "    ins = self.layernorm(ins)\n",
        "    ins = self.MHA(ins)\n",
        "    ins = self.post_attention_dropout(ins)\n",
        "    ins += residual\n",
        "\n",
        "    residual2 = ins.clone()\n",
        "    ins = self.layernorm(ins)\n",
        "    ins = self.MLP(ins)\n",
        "    ins = self.post_MLP_dropout(ins)\n",
        "    ins += residual2\n",
        "    return ins\n"
      ],
      "metadata": {
        "id": "NnKYzMWbMPcR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, n_heads, emb_dim, max_seq_len, n_blocks, tokenizer, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.emb_dim = emb_dim\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.n_blocks = n_blocks\n",
        "    self.tokenizer = tokenizer\n",
        "    self.layernorm = nn.LayerNorm(emb_dim)\n",
        "    self.emb = nn.Embedding(len(tokenizer) + 1, emb_dim)\n",
        "    self.pos_encoding_matrix = create_positional_encoding_matrix(max_seq_len, emb_dim).to(device)\n",
        "    self.decoder_stack = nn.ModuleList([\n",
        "        TransformerBlock(n_heads, emb_dim, dropout=dropout)\n",
        "        for _ in range(n_blocks)\n",
        "    ])\n",
        "\n",
        "    self.logits_layer = nn.Linear(emb_dim, len(tokenizer)+1)\n",
        "  def forward(self, ins):\n",
        "    embedding = self.emb(ins)\n",
        "    pos_encoding_indices = torch.arange(ins.shape[1]).to(device) # seq_len\n",
        "\n",
        "    x = embedding\n",
        "    x += self.pos_encoding_matrix[pos_encoding_indices]\n",
        "\n",
        "    for decoder in self.decoder_stack:\n",
        "      x = decoder(x)\n",
        "\n",
        "    x = self.layernorm(x)\n",
        "    logits = self.logits_layer(x)\n",
        "    return logits\n",
        "  @torch.no_grad()\n",
        "  def generate(self, text, length=10):\n",
        "    out = \"\"\n",
        "    for x in range(length):\n",
        "      encodings = torch.tensor(self.tokenizer.encode(text + out)).to(device)\n",
        "      seq_len = len(encodings)\n",
        "      if seq_len > self.max_seq_len:\n",
        "        encodings = encodings[len(encodings) - self.max_seq_len:]\n",
        "      tokens = encodings.view(1, -1)\n",
        "      logits = self.forward(tokens)[0, -1, :] # logits for last char\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      selection_index = torch.multinomial(probs, num_samples=1)\n",
        "      out += self.tokenizer.decode([selection_index.item()])\n",
        "    return out"
      ],
      "metadata": {
        "id": "GEt7OmRIPcGP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 256\n",
        "emb_dim = 384\n",
        "n_heads = 6\n",
        "n_blocks = 6\n",
        "dropout = 0.25\n",
        "\n",
        "lr = 0.001\n",
        "batch_size = 8\n",
        "max_steps = 2001\n",
        "\n",
        "log_period = 50\n",
        "val_interval = 500\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "myGPT = GPT(n_heads, emb_dim, max_seq_len, n_blocks, tokenizer, dropout=dropout)\n",
        "myGPT = myGPT.to(device)\n",
        "print(\"Total params:\", sum(p.numel() for p in myGPT.parameters()))\n",
        "if torch.cuda.is_available():\n",
        "  myGPT = torch.compile(myGPT)\n",
        "optim = torch.optim.AdamW(myGPT.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgwFWjqEiTUr",
        "outputId": "05862fd1-3d37-4ea7-f210-3e6bcca8e0ae"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 10693698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(max_seq_len, tokens):\n",
        "  context_x = tokens[:max_seq_len]\n",
        "  context_y = tokens[1:max_seq_len+1]\n",
        "  x_examples = [context_x]\n",
        "  y_examples = [context_y]\n",
        "  for token, next in zip(tokens[max_seq_len:], tokens[max_seq_len+1:]):\n",
        "    context_x = context_x[1:]\n",
        "    context_x.append(token)\n",
        "    context_y = context_y[1:]\n",
        "    context_y.append(next)\n",
        "    x_examples.append(context_x)\n",
        "    y_examples.append(context_y)\n",
        "  return torch.tensor(x_examples).long().to(device), torch.tensor(y_examples).to(device)\n",
        "tokenized_train_data = tokenizer.encode(train_text)\n",
        "Xtr, Ytr = generate_dataset(max_seq_len, tokenized_train_data)\n",
        "tokenized_test_data = tokenizer.encode(val_text)\n",
        "Xte, Yte = generate_dataset(max_seq_len, tokenized_test_data)\n",
        "tokenizer.decode(Xtr[0].cpu().numpy()), tokenizer.decode(Ytr[0].cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atTGrKNsfhZn",
        "outputId": "4517658c-3567-4eb2-cb59-aa16a1098e2a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\n',\n",
              " 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nW')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_len = len(Xtr) - 1\n",
        "best_val_loss = float('inf')\n",
        "import time\n",
        "for step in range(max_steps):\n",
        "  step_start = time.time()\n",
        "  batch_indices = torch.randint(0, train_len, (batch_size,)).to(device)\n",
        "  x_batch = Xtr[batch_indices]\n",
        "  y_batch = Ytr[batch_indices].view(-1)\n",
        "\n",
        "  logits = myGPT(x_batch)\n",
        "  logits = logits.view(-1, logits.size(-1))\n",
        "  loss = F.cross_entropy(logits, y_batch)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if step % log_period == 0:\n",
        "    with torch.no_grad():\n",
        "      if step % val_interval == 0:\n",
        "        validation_loss = 0\n",
        "        for _ in range(len(Xte)//batch_size):\n",
        "          batch_indices = torch.randint(0, len(Xte), (batch_size,)).to(device)\n",
        "          x_batch = Xte[batch_indices]\n",
        "          y_batch = Yte[batch_indices].view(-1)\n",
        "          gpt_predictions = myGPT(x_batch)\n",
        "          gpt_predictions = gpt_predictions.view(-1, gpt_predictions.size(-1))\n",
        "          current_loss = F.cross_entropy(gpt_predictions, y_batch.view(-1))\n",
        "          validation_loss += current_loss\n",
        "        validation_loss /= len(Xte)//batch_size\n",
        "        if validation_loss < best_val_loss:\n",
        "          best_val_loss = validation_loss\n",
        "          torch.save(myGPT.state_dict(), \"best_model.pt\")\n",
        "        print(\"Step\", str(step) + \", val loss:\", validation_loss.item())\n",
        "    step_end = time.time()\n",
        "    step_time = step_end - step_start\n",
        "    print(f\"Step {step} ({step_time * 1e3} ms: loss {loss.item()})\")\n",
        "print(\"Loading best model\")\n",
        "myGPT.load_state_dict(torch.load(\"best_model.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5esfI5aiapD",
        "outputId": "5357d7c1-1440-48a1-933f-6f9b7d739207"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, val loss: 3.4984593391418457\n",
            "Step 0 (286574.8918056488 ms: loss 4.404128551483154)\n",
            "Step 50 (51.773786544799805 ms: loss 2.525001049041748)\n",
            "Step 100 (50.72021484375 ms: loss 2.579176187515259)\n",
            "Step 150 (50.60386657714844 ms: loss 2.507404088973999)\n",
            "Step 200 (48.41423034667969 ms: loss 2.3467252254486084)\n",
            "Step 250 (49.44133758544922 ms: loss 2.04978084564209)\n",
            "Step 300 (53.29179763793945 ms: loss 1.5433398485183716)\n",
            "Step 350 (50.11105537414551 ms: loss 1.3528873920440674)\n",
            "Step 400 (59.16166305541992 ms: loss 1.2884275913238525)\n",
            "Step 450 (55.155038833618164 ms: loss 1.26047945022583)\n",
            "Step 500, val loss: 1.2518144845962524\n",
            "Step 500 (242969.34342384338 ms: loss 1.2306936979293823)\n",
            "Step 550 (51.078081130981445 ms: loss 1.2474048137664795)\n",
            "Step 600 (50.56881904602051 ms: loss 1.222843050956726)\n",
            "Step 650 (51.57876014709473 ms: loss 1.2023743391036987)\n",
            "Step 700 (51.38349533081055 ms: loss 1.221926212310791)\n",
            "Step 750 (52.72507667541504 ms: loss 1.2199162244796753)\n",
            "Step 800 (60.796260833740234 ms: loss 1.1828482151031494)\n",
            "Step 850 (52.66523361206055 ms: loss 1.2616668939590454)\n",
            "Step 900 (52.43325233459473 ms: loss 1.211296558380127)\n",
            "Step 950 (53.34973335266113 ms: loss 1.2419254779815674)\n",
            "Step 1000, val loss: 1.2285016775131226\n",
            "Step 1000 (242128.58390808105 ms: loss 1.2043261528015137)\n",
            "Step 1050 (38.81216049194336 ms: loss 1.2086765766143799)\n",
            "Step 1100 (50.60410499572754 ms: loss 1.2255024909973145)\n",
            "Step 1150 (53.73954772949219 ms: loss 1.2097638845443726)\n",
            "Step 1200 (51.1631965637207 ms: loss 1.2292097806930542)\n",
            "Step 1250 (53.140878677368164 ms: loss 1.1776760816574097)\n",
            "Step 1300 (50.86851119995117 ms: loss 1.2336921691894531)\n",
            "Step 1350 (51.68962478637695 ms: loss 1.2115963697433472)\n",
            "Step 1400 (50.41313171386719 ms: loss 1.2141324281692505)\n",
            "Step 1450 (50.519704818725586 ms: loss 1.1854830980300903)\n",
            "Step 1500, val loss: 1.2134727239608765\n",
            "Step 1500 (243125.20003318787 ms: loss 1.1828396320343018)\n",
            "Step 1550 (51.05948448181152 ms: loss 1.1943703889846802)\n",
            "Step 1600 (52.175045013427734 ms: loss 1.1965831518173218)\n",
            "Step 1650 (52.8109073638916 ms: loss 1.2123136520385742)\n",
            "Step 1700 (51.90706253051758 ms: loss 1.1751506328582764)\n",
            "Step 1750 (50.9943962097168 ms: loss 1.250141978263855)\n",
            "Step 1800 (53.05171012878418 ms: loss 1.1802959442138672)\n",
            "Step 1850 (60.24026870727539 ms: loss 1.2190545797348022)\n",
            "Step 1900 (56.273460388183594 ms: loss 1.1540390253067017)\n",
            "Step 1950 (46.24319076538086 ms: loss 1.1608526706695557)\n",
            "Step 2000, val loss: 1.115155816078186\n",
            "Step 2000 (243539.38817977905 ms: loss 1.092125654220581)\n",
            "Loading best model\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(myGPT.generate(\"\\n\", length=500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eEOVX6D20m3",
        "outputId": "35aeef2a-3cac-454a-9979-b1fdc6a43ca6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "bl's bicaledyovintve gale o lde ser wabu haire y ceand hen haveme tounghthethenentivete te t ulece whe cceathen' thenth he wnceetheethe tee theetheveve t hit te hle Itha tche tot cend toncthineane:\n",
            "Wht,\n",
            "Wh!\n",
            "Whe f theete netom s, thewhe methe, t thanervent qut. thtom thithy t\n",
            "Catout ovy, f foungr;\n",
            "Myor,-favas ssse s ckerthereacenensesenerexceverthexngrint ke\n",
            "THet.\n",
            "BRCHAD ESick?\n",
            "Whot\n",
            "KIZI\n",
            "\n",
            "\n",
            "COUCOLLANEDYFF t.\n",
            "\n",
            "TUCKINCKELICKI C\n",
            "\n",
            "HIURDWIOWBUKING VINUCLABESRD HELIULI:\n",
            "BRDONUMING KICY IFWh.\n",
            "QUCHADWh. \n"
          ]
        }
      ]
    }
  ]
}