{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMB4VortRx7qpFbWIufxcCF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trickshotblaster/nn-practices/blob/main/TransformerV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mF1Wi04gwqoL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "raw_text = requests.get(url).text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_amount = 0.9\n",
        "train_text, val_text = raw_text[:int(len(raw_text) * train_amount)], raw_text[int(len(raw_text) * train_amount):]\n",
        "train_text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ARWYIXl9xFNT",
        "outputId": "8d9da1ef-89af-417a-9bf4-f551968173a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(raw_text)\n",
        "vocab.add(\"<|UNK|>\")\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "stoi = {char:i for i, char in enumerate(vocab)}\n",
        "itos = {i:char for i, char in enumerate(stoi)}\n",
        "itos[stoi['<|UNK|>']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YY3cBAtMxaNW",
        "outputId": "7962c649-b714-483e-af7e-6d1841a3db79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|UNK|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, stoi, itos):\n",
        "    self.stoi = stoi\n",
        "    self.itos = itos\n",
        "  def __len__(self):\n",
        "    return len(self.stoi) - 1\n",
        "  def encode(self, text):\n",
        "    return [stoi.get(char, stoi['<|UNK|>']) for char in text]\n",
        "  def decode(self, tokens):\n",
        "    return \"\".join([itos.get(token, '<|UNK|>') for token in tokens])\n",
        "tokenizer = Tokenizer(stoi, itos)\n",
        "tokenizer.decode(tokenizer.encode(\"hello world ^}|%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2tqyzMtdyA44",
        "outputId": "db262e72-af57-4008-a125-8c56867cf426"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world <|UNK|><|UNK|><|UNK|><|UNK|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "GKc4Oey01Gif"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 16\n",
        "emb_dim = 32\n",
        "import math\n",
        "def create_positional_encoding_matrix(max_seq_len, emb_dim):\n",
        "  mask = torch.zeros((max_seq_len, emb_dim))\n",
        "  for pos in range(max_seq_len):\n",
        "    for i in range(0, emb_dim, 2):\n",
        "      mask[pos, i] = math.sin(pos/(10000 ** ((2*i)/emb_dim)))\n",
        "      mask[pos, i+1] = math.cos(pos/(10000 ** ((2*(i+1))/emb_dim)))\n",
        "  return mask\n",
        "positional_encoding_matrix = create_positional_encoding_matrix(max_seq_len, emb_dim)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(positional_encoding_matrix.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "rhLazuVi0Xs3",
        "outputId": "59a187fc-6d85-4cc9-ab45-b08eb54a2628"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5cf42727a0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEjCAYAAACSDWOaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAceElEQVR4nO3de3BU9f3/8dfmtkFIFsIll5JAvBQqlygomZTWgmTE/JRi7UVbalN0wEtQEWshnQJeqlHbcfDCQGt/FTqjgPYraP3+xAtyqS2gSaBqL5FoClFIov7KBoJZYvbz+8Of228kgLvn7Ofshudj5syw53w+vN/99FhfPXvOHp8xxggAAMCSFK8bAAAApxbCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsSvO6gc8Lh8Pav3+/srKy5PP5vG4HAAB8AcYYHTp0SAUFBUpJOfG1jYQLH/v371dhYaHXbQAAgBg0Nzdr+PDhJxyTcOEjKytLkjR86c+VkpkZ9fy/fvt3juqX/NfVMc+lNrWpTW1qU/tUrd1+OKwRE/4V+ff4iSRc+Pjsq5aUzMyYwkd2lrPbWGKpSW1qU5va1KY2tT/1RW6Z4IZTAABgVdzCx/LlyzVy5EhlZmaqtLRUr732WrxKAQCAJBKX8LFu3TotWLBAS5cuVX19vUpKSjR9+nS1tbXFoxwAAEgicQkfDzzwgObMmaPZs2fr7LPP1sqVK3Xaaafpd79zdhMNAABIfq6Hj6NHj6qurk7l5eX/KZKSovLycm3fvv2Y8aFQSO3t7T02AADQd7kePj788EN1d3crNze3x/7c3Fy1tLQcM76mpkaBQCCy8RsfAAD0bZ4/7VJdXa1gMBjZmpubvW4JAADEkeu/8zFkyBClpqaqtbW1x/7W1lbl5eUdM97v98vv97vdBgAASFCuX/nIyMjQxIkTtWnTpsi+cDisTZs2qayszO1yAAAgycTlF04XLFigyspKnXfeeZo0aZKWLVumjo4OzZ49Ox7lAABAEolL+Ljiiiv0wQcfaMmSJWppadE555yjjRs3HnMTKgAAOPXE7d0u8+bN07x58+L11wMAgCSVcC+W+8yqS1dqQAwvuNkdMo7qDjzr/8Y8t8t0O6odHuBsvhMm3dm6OeLlM1cnf/8Rtamd/LWBBOP5o7YAAODUQvgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWpXndwPEMSz2qrNTos9ENTd9xVHf68H/GPDcY7nRUO21Al6P5Tpg0411tXjUOxBf/jCHBcOUDAABYRfgAAABWET4AAIBVhA8AAGCV6+GjpqZG559/vrKysjRs2DBddtllamhocLsMAABIUq6Hj61bt6qqqko7duzQSy+9pK6uLl100UXq6OhwuxQAAEhCrj9qu3Hjxh6fV61apWHDhqmurk4XXHCB2+UAAECSifvvfASDQUlSTk5Or8dDoZBCoVDkc3t7e7xbAgAAHorrDafhcFjz58/X5MmTNXbs2F7H1NTUKBAIRLbCwsJ4tgQAADwW1/BRVVWlt956S2vXrj3umOrqagWDwcjW3Nwcz5YAAIDH4va1y7x58/Tcc89p27ZtGj58+HHH+f1++f3+eLUBAAASjOvhwxijG2+8UevXr9eWLVtUXFzsdgkAAJDEXA8fVVVVeuKJJ/TMM88oKytLLS0tkqRAIKB+/fq5XQ4AACQZ1+/5WLFihYLBoKZMmaL8/PzItm7dOrdLAQCAJBSXr10AAACOJ+6/8xGrim3XK6VfZtTzMvY6u3l14VX/J+a5rd3OLiQN6N8Z89xuE3ZUW6kehkafl7W9Kw0ApypeLAcAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKvSvG7geL687LDSUruintc12Nmr5UuuORrz3Jc/Huqodk7/IzHP/UTdjmorzcPX2hOBAeCUwv/sAwAAqwgfAADAKsIHAACwivABAACsinv4uPfee+Xz+TR//vx4lwIAAEkgruHj9ddf169//WuNHz8+nmUAAEASiVv4OHz4sGbNmqVHH31UgwYNilcZAACQZOIWPqqqqnTJJZeovLz8hONCoZDa29t7bAAAoO+Ky4+MrV27VvX19Xr99ddPOrampkZ33HFHPNoAAAAJyPUrH83Nzbr55pv1+OOPKzMz86Tjq6urFQwGI1tzc7PbLQEAgATi+pWPuro6tbW1acKECZF93d3d2rZtmx555BGFQiGlpqZGjvn9fvn9frfbAAAACcr18DFt2jS9+eabPfbNnj1bo0eP1sKFC3sEDwAAcOpxPXxkZWVp7NixPfb1799fgwcPPmY/AAA49fALpwAAwKq4PO3yeVu2bLFRBgAAJAEr4SMW4aZmhX3pUc9Le9fnqO6AlJM/oXM8f//4S45q5/Y7FPPcLtPtqLYvLexovrPi3pU2HtYGTgn8M4Ze8LULAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsSvO6geNp/fE5SvVH/3r7vBWvOaobDH8c89y3DhU4qp2fGYx5bpcJO6qdkuZsvhPGZzyrDQCwjysfAADAKsIHAACwivABAACsInwAAACr4hI+3n//ff3whz/U4MGD1a9fP40bN061tbXxKAUAAJKM60+7/Pvf/9bkyZM1depUPf/88xo6dKj27NmjQYMGuV0KAAAkIdfDx3333afCwkI99thjkX3FxcVulwEAAEnK9a9dnn32WZ133nn67ne/q2HDhuncc8/Vo48+etzxoVBI7e3tPTYAANB3uR4+3n33Xa1YsUJnnXWWXnjhBV1//fW66aabtHr16l7H19TUKBAIRLbCwkK3WwIAAAnE9fARDoc1YcIE3XPPPTr33HM1d+5czZkzRytXrux1fHV1tYLBYGRrbm52uyUAAJBAXA8f+fn5Ovvss3vs+8pXvqJ9+/b1Ot7v9ys7O7vHBgAA+i7Xw8fkyZPV0NDQY9/bb7+tESNGuF0KAAAkIdfDxy233KIdO3bonnvuUWNjo5544gn95je/UVVVldulAABAEnI9fJx//vlav3691qxZo7Fjx+quu+7SsmXLNGvWLLdLAQCAJOT673xI0qWXXqpLL700Hn81AABIcnEJH26Ydc2LyhwQfXsb/zLZUd1dofqY5+7591BHtc8d2ftNuV9El4yj2qmpYUfzHfF5VxoAYB8vlgMAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFXRv7PekmsD/1J2VvTZ6H9f+L8c1f3vYEnMcz/6MMtR7dwzgzHP7TTGUe20tO6Y53absKPa8jmb7qy2s3VzVtu70gDgJa58AAAAqwgfAADAKsIHAACwivABAACscj18dHd3a/HixSouLla/fv10xhln6K677pJxeEMkAADoG1x/2uW+++7TihUrtHr1ao0ZM0a1tbWaPXu2AoGAbrrpJrfLAQCAJON6+PjLX/6imTNn6pJLLpEkjRw5UmvWrNFrr73mdikAAJCEXP/a5atf/ao2bdqkt99+W5L017/+Va+++qoqKip6HR8KhdTe3t5jAwAAfZfrVz4WLVqk9vZ2jR49Wqmpqeru7tbdd9+tWbNm9Tq+pqZGd9xxh9ttAACABOX6lY8nn3xSjz/+uJ544gnV19dr9erV+tWvfqXVq1f3Or66ulrBYDCyNTc3u90SAABIIK5f+bjtttu0aNEiXXnllZKkcePGae/evaqpqVFlZeUx4/1+v/x+v9ttAACABOX6lY8jR44oJaXnX5uamqpw2OH7PwAAQJ/g+pWPGTNm6O6771ZRUZHGjBmjXbt26YEHHtDVV1/tdikAAJCEXA8fDz/8sBYvXqwbbrhBbW1tKigo0LXXXqslS5a4XQoAACQh18NHVlaWli1bpmXLlrn9VwMAgD7A9fDhlu/uuVhp/aO/EXXAha2O6r7cPCrmuSkfZDiqnZcWjHluR9jZ7TupqR7ek+Pjp/cB4FTCi+UAAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFiV5nUDx9P5UL7S0jOjnveLh3/nqO616+fEPLffBz5HtQendsQ8N2RSHdVOT+2OeW5YxlFtOVu25K0NAKcornwAAACrCB8AAMAqwgcAALAq6vCxbds2zZgxQwUFBfL5fNqwYUOP48YYLVmyRPn5+erXr5/Ky8u1Z88et/oFAABJLurw0dHRoZKSEi1fvrzX4/fff78eeughrVy5Ujt37lT//v01ffp0dXZ2Om4WAAAkv6ifdqmoqFBFRUWvx4wxWrZsmX7+859r5syZkqTf//73ys3N1YYNG3TllVc66xYAACQ9V+/5aGpqUktLi8rLyyP7AoGASktLtX379l7nhEIhtbe399gAAEDf5Wr4aGlpkSTl5ub22J+bmxs59nk1NTUKBAKRrbCw0M2WAABAgvH8aZfq6moFg8HI1tzc7HVLAAAgjlwNH3l5eZKk1tbWHvtbW1sjxz7P7/crOzu7xwYAAPouV8NHcXGx8vLytGnTpsi+9vZ27dy5U2VlZW6WAgAASSrqp10OHz6sxsbGyOempibt3r1bOTk5Kioq0vz58/WLX/xCZ511loqLi7V48WIVFBTosssuc7NvAACQpKIOH7W1tZo6dWrk84IFCyRJlZWVWrVqlX7605+qo6NDc+fO1cGDB/W1r31NGzduVGZm9C+JAwAAfU/U4WPKlCky5vhvMfX5fLrzzjt15513OmoMAAD0TVGHD1v8L9YrzZce9bxpv4391fCSFHg79nespx519mr5nJSjMc/d332ao9oZac7WzZEUZ+sGAEgunj9qCwAATi2EDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWJXmdQPHE5o+Qd3pmVHPa+p61VHdQXtCMc/95LRUR7WzUnwxzz3U1c9RbX9qt6P5jsT+HxsAkIS48gEAAKwifAAAAKsIHwAAwKqow8e2bds0Y8YMFRQUyOfzacOGDZFjXV1dWrhwocaNG6f+/furoKBAP/rRj7R//343ewYAAEks6vDR0dGhkpISLV++/JhjR44cUX19vRYvXqz6+no9/fTTamho0De/+U1XmgUAAMkv6qddKioqVFFR0euxQCCgl156qce+Rx55RJMmTdK+fftUVFQUW5cAAKDPiPujtsFgUD6fTwMHDuz1eCgUUij0n8db29vb490SAADwUFxvOO3s7NTChQv1/e9/X9nZ2b2OqampUSAQiGyFhYXxbAkAAHgsbuGjq6tL3/ve92SM0YoVK447rrq6WsFgMLI1NzfHqyUAAJAA4vK1y2fBY+/evXrllVeOe9VDkvx+v/x+fzzaAAAACcj18PFZ8NizZ482b96swYMHu10CAAAksajDx+HDh9XY2Bj53NTUpN27dysnJ0f5+fn6zne+o/r6ej333HPq7u5WS0uLJCknJ0cZGRnudQ4AAJJS1OGjtrZWU6dOjXxesGCBJKmyslK33367nn32WUnSOeec02Pe5s2bNWXKlNg7BQAAfULU4WPKlCkyxhz3+ImOAQAAxP13PmJ12k37ldY/+htR72mZ7qiu/90PYp6bmjvQUe3TfOkxz+0IO7tp15/6Scxzwwo7qi2fs+lJWxsATlG8WA4AAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFVpXjdwPOvOfEHZWdFno1FrbnBU98yWXTHPTU13tpx+X+zzD4UzHdVOT+12NN8Rn/GuNgDAOq58AAAAqwgfAADAKsIHAACwKurwsW3bNs2YMUMFBQXy+XzasGHDccded9118vl8WrZsmYMWAQBAXxJ1+Ojo6FBJSYmWL19+wnHr16/Xjh07VFBQEHNzAACg74n68YqKigpVVFSccMz777+vG2+8US+88IIuueSSmJsDAAB9j+uP2obDYV111VW67bbbNGbMmJOOD4VCCoVCkc/t7e1utwQAABKI6zec3nfffUpLS9NNN930hcbX1NQoEAhEtsLCQrdbAgAACcTV8FFXV6cHH3xQq1atks/n+0JzqqurFQwGI1tzc7ObLQEAgATjavj405/+pLa2NhUVFSktLU1paWnau3evbr31Vo0cObLXOX6/X9nZ2T02AADQd7l6z8dVV12l8vLyHvumT5+uq666SrNnz3azFAAASFJRh4/Dhw+rsbEx8rmpqUm7d+9WTk6OioqKNHjw4B7j09PTlZeXp1GjRjnvFgAAJL2ow0dtba2mTp0a+bxgwQJJUmVlpVatWuVaYwAAoG+KOnxMmTJFxnzxt5D+61//irYEAADow1z/nQ+3rDh4ujI/ib694ZucvRre/I/fHInaQWe/UZLqi/3+30Pd/RzVzkztinludxRhtDe+FGfznRX3rjQAnKp4sRwAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKyK/p31cWb+/+vZOw9/EtP8T7o6HdVPNbG/Wt6Ejzqq3X4oHPPcWNfrM10dsffupG9JCn8c+39nSV27k9rUpja1+07t9sOfzv3s3+Mn4jNfZJRF7733ngoLC71uAwAAxKC5uVnDhw8/4ZiECx/hcFj79+9XVlaWfD7fMcfb29tVWFio5uZmZWdne9BhcmLdoseaxYZ1ix5rFhvWLXrxXDNjjA4dOqSCggKlpJz4ro6E+9olJSXlpIlJkrKzsznZYsC6RY81iw3rFj3WLDasW/TitWaBQOALjeOGUwAAYBXhAwAAWJV04cPv92vp0qXy+/1et5JUWLfosWaxYd2ix5rFhnWLXqKsWcLdcAoAAPq2pLvyAQAAkhvhAwAAWEX4AAAAVhE+AACAVYQPAABgVdKFj+XLl2vkyJHKzMxUaWmpXnvtNa9bSli33367fD5fj2306NFet5Vwtm3bphkzZqigoEA+n08bNmzocdwYoyVLlig/P1/9+vVTeXm59uzZ402zCeJka/bjH//4mHPv4osv9qbZBFFTU6Pzzz9fWVlZGjZsmC677DI1NDT0GNPZ2amqqioNHjxYAwYM0Le//W21trZ61HFi+CLrNmXKlGPOt+uuu86jjr23YsUKjR8/PvIrpmVlZXr++ecjxxPhPEuq8LFu3TotWLBAS5cuVX19vUpKSjR9+nS1tbV53VrCGjNmjA4cOBDZXn31Va9bSjgdHR0qKSnR8uXLez1+//3366GHHtLKlSu1c+dO9e/fX9OnT1engzdHJruTrZkkXXzxxT3OvTVr1ljsMPFs3bpVVVVV2rFjh1566SV1dXXpoosuUkdHR2TMLbfcoj/+8Y966qmntHXrVu3fv1+XX365h11774usmyTNmTOnx/l2//33e9Sx94YPH657771XdXV1qq2t1YUXXqiZM2fqb3/7m6QEOc9MEpk0aZKpqqqKfO7u7jYFBQWmpqbGw64S19KlS01JSYnXbSQVSWb9+vWRz+Fw2OTl5Zlf/vKXkX0HDx40fr/frFmzxoMOE8/n18wYYyorK83MmTM96SdZtLW1GUlm69atxphPz6v09HTz1FNPRcb84x//MJLM9u3bvWoz4Xx+3Ywx5hvf+Ia5+eabvWsqCQwaNMj89re/TZjzLGmufBw9elR1dXUqLy+P7EtJSVF5ebm2b9/uYWeJbc+ePSooKNDpp5+uWbNmad++fV63lFSamprU0tLS47wLBAIqLS3lvDuJLVu2aNiwYRo1apSuv/56ffTRR163lFCCwaAkKScnR5JUV1enrq6uHufa6NGjVVRUxLn2P3x+3T7z+OOPa8iQIRo7dqyqq6t15MgRL9pLON3d3Vq7dq06OjpUVlaWMOdZwr3V9ng+/PBDdXd3Kzc3t8f+3Nxc/fOf//Soq8RWWlqqVatWadSoUTpw4IDuuOMOff3rX9dbb72lrKwsr9tLCi0tLZLU63n32TEc6+KLL9bll1+u4uJivfPOO/rZz36miooKbd++XampqV6357lwOKz58+dr8uTJGjt2rKRPz7WMjAwNHDiwx1jOtf/obd0k6Qc/+IFGjBihgoICvfHGG1q4cKEaGhr09NNPe9itt958802VlZWps7NTAwYM0Pr163X22Wdr9+7dCXGeJU34QPQqKioifx4/frxKS0s1YsQIPfnkk7rmmms87Ax93ZVXXhn587hx4zR+/HidccYZ2rJli6ZNm+ZhZ4mhqqpKb731FvdgRel46zZ37tzIn8eNG6f8/HxNmzZN77zzjs444wzbbSaEUaNGaffu3QoGg/rDH/6gyspKbd261eu2IpLma5chQ4YoNTX1mDtyW1tblZeX51FXyWXgwIH68pe/rMbGRq9bSRqfnVucd86cfvrpGjJkCOeepHnz5um5557T5s2bNXz48Mj+vLw8HT16VAcPHuwxnnPtU8dbt96UlpZK0il9vmVkZOjMM8/UxIkTVVNTo5KSEj344IMJc54lTfjIyMjQxIkTtWnTpsi+cDisTZs2qayszMPOksfhw4f1zjvvKD8/3+tWkkZxcbHy8vJ6nHft7e3auXMn510U3nvvPX300Uen9LlnjNG8efO0fv16vfLKKyouLu5xfOLEiUpPT+9xrjU0NGjfvn2n9Ll2snXrze7duyXplD7fPi8cDisUCiXOeWbt1lYXrF271vj9frNq1Srz97//3cydO9cMHDjQtLS0eN1aQrr11lvNli1bTFNTk/nzn/9sysvLzZAhQ0xbW5vXrSWUQ4cOmV27dpldu3YZSeaBBx4wu3btMnv37jXGGHPvvfeagQMHmmeeeca88cYbZubMmaa4uNh8/PHHHnfunROt2aFDh8xPfvITs337dtPU1GRefvllM2HCBHPWWWeZzs5Or1v3zPXXX28CgYDZsmWLOXDgQGQ7cuRIZMx1111nioqKzCuvvGJqa2tNWVmZKSsr87Br751s3RobG82dd95pamtrTVNTk3nmmWfM6aefbi644AKPO/fOokWLzNatW01TU5N54403zKJFi4zP5zMvvviiMSYxzrOkCh/GGPPwww+boqIik5GRYSZNmmR27NjhdUsJ64orrjD5+fkmIyPDfOlLXzJXXHGFaWxs9LqthLN582Yj6ZitsrLSGPPp47aLFy82ubm5xu/3m2nTppmGhgZvm/bYidbsyJEj5qKLLjJDhw416enpZsSIEWbOnDmn/P9J6G29JJnHHnssMubjjz82N9xwgxk0aJA57bTTzLe+9S1z4MAB75pOACdbt3379pkLLrjA5OTkGL/fb84880xz2223mWAw6G3jHrr66qvNiBEjTEZGhhk6dKiZNm1aJHgYkxjnmc8YY+xdZwEAAKe6pLnnAwAA9A2EDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFj1/wCa7ITtN9MeXQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedMultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_heads, emb_dim):\n",
        "    super().__init__()\n",
        "    assert emb_dim % n_heads == 0, \"Embedding dimension must be divisible by head number\"\n",
        "    self.n_heads = n_heads\n",
        "    self.emb_dim = emb_dim\n",
        "    self.head_size = emb_dim // n_heads\n",
        "    self.wq = nn.Linear(emb_dim, emb_dim)\n",
        "    self.wk = nn.Linear(emb_dim, emb_dim)\n",
        "    self.wv = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    self.wo = nn.Linear(emb_dim, emb_dim)\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len, emb_dim = x.size()\n",
        "    Q = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_size).transpose(1, 2)\n",
        "    K = self.wk(x).view(batch_size, seq_len, self.n_heads, self.head_size).transpose(1, 2)\n",
        "    V = self.wv(x).view(batch_size, seq_len, self.n_heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "    scaled_dot_product = (torch.matmul(Q, K.transpose(2, 3)))/(self.head_size**0.5)\n",
        "\n",
        "    attn_mask = torch.triu(\n",
        "        torch.ones(seq_len, seq_len),\n",
        "        diagonal=1\n",
        "    )\n",
        "    attn_mask.masked_fill_(attn_mask==1, float('-inf'))\n",
        "\n",
        "    attn_scores = torch.matmul(F.softmax(scaled_dot_product, dim=-1), V)\n",
        "    attn_scores = attn_scores.view(batch_size, -1, self.n_heads * self.head_size)\n",
        "\n",
        "    out = self.wo(attn_scores)\n",
        "\n",
        "    return out # (batch_size, seq_len, emb_dim)\n",
        "mha = MaskedMultiHeadAttention(8, emb_dim)\n",
        "mha(torch.randn(1, max_seq_len, emb_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-Mcx23B5lFO",
        "outputId": "5a2051f3-1957-4ce0-f622-9812911780f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.1079e-01, -5.2996e-02, -1.2542e-01,  2.0740e-02,  1.9853e-01,\n",
              "          -4.7637e-03,  2.4665e-01,  1.7415e-01,  1.4261e-01, -9.9614e-02,\n",
              "           9.4028e-02, -1.7790e-01, -7.4708e-04,  9.3606e-02,  1.5862e-01,\n",
              "           1.4062e-01,  4.7044e-02,  1.2322e-01, -1.2288e-01,  1.0566e-01,\n",
              "          -5.8030e-02, -3.2190e-01,  7.2645e-03, -5.5529e-02,  4.6495e-03,\n",
              "           1.0663e-01,  2.0339e-02,  6.5438e-03,  1.3645e-01,  3.1781e-02,\n",
              "           1.1376e-01,  4.0381e-02],\n",
              "         [-1.0643e-01, -4.3847e-02, -1.2161e-01, -4.5406e-03,  1.7211e-01,\n",
              "          -5.9603e-02,  2.0274e-01,  1.6685e-01,  8.2365e-02, -1.1966e-01,\n",
              "           3.2123e-02, -1.9022e-01, -2.4701e-02,  7.1272e-02,  1.1387e-01,\n",
              "           1.3588e-01,  1.6969e-04,  1.6315e-01, -6.8856e-02,  5.4099e-02,\n",
              "          -7.3840e-02, -3.1247e-01,  1.9836e-02, -7.9970e-02,  5.0794e-02,\n",
              "           1.1727e-01,  7.7811e-02,  1.2638e-02,  6.2088e-02,  8.2456e-02,\n",
              "           4.1957e-02,  4.6338e-02],\n",
              "         [-5.0011e-02, -4.6216e-02, -1.2989e-01,  3.6947e-02,  1.9802e-01,\n",
              "          -1.0075e-01,  1.2645e-01,  2.3166e-01,  1.5275e-01, -5.0080e-02,\n",
              "           6.8164e-02, -6.8394e-02, -1.4143e-02, -3.6619e-02,  8.1863e-02,\n",
              "           1.9456e-01,  1.4400e-02,  8.8398e-02, -2.5646e-02,  6.6466e-02,\n",
              "          -4.0571e-02, -3.4040e-01,  1.4790e-01, -1.6701e-01,  2.8208e-02,\n",
              "           1.3471e-01,  1.0669e-01,  5.0039e-02,  1.1141e-01,  1.7027e-01,\n",
              "           9.8079e-03, -1.3209e-02],\n",
              "         [-9.7043e-02, -6.1567e-02, -1.8254e-01,  1.1277e-01,  8.7494e-02,\n",
              "           3.4224e-02,  8.8713e-02,  2.1286e-01,  9.2358e-02,  1.6782e-02,\n",
              "           2.0832e-01, -6.4504e-02, -8.8379e-02, -1.1594e-01,  1.2193e-01,\n",
              "           1.6461e-01, -4.1957e-02,  3.2046e-02, -4.0967e-02,  7.6007e-02,\n",
              "           1.3066e-02, -1.7994e-01,  2.0664e-02, -1.6420e-01, -1.9474e-02,\n",
              "           1.7281e-01,  1.4406e-01,  3.4018e-02,  9.5346e-02,  2.2646e-01,\n",
              "          -6.6426e-02, -6.2479e-02],\n",
              "         [-8.1806e-02,  1.0404e-03, -1.9701e-01, -1.5295e-02,  5.8342e-02,\n",
              "           7.1648e-02, -9.6213e-02,  2.1770e-01, -1.3351e-01, -2.4671e-01,\n",
              "          -4.0740e-02, -8.7545e-02, -7.3326e-02,  1.5940e-02,  1.1619e-01,\n",
              "           1.3293e-01, -1.5564e-01,  2.7405e-01,  9.8363e-02,  1.1080e-01,\n",
              "          -2.0553e-02, -1.6503e-01,  9.8395e-02, -1.5800e-01, -1.7106e-02,\n",
              "           6.8286e-02,  3.0169e-01, -7.5575e-03, -7.0773e-02,  2.5071e-01,\n",
              "           9.0708e-02,  1.9986e-01],\n",
              "         [-7.0695e-02, -1.0693e-02, -1.8100e-01,  4.0576e-03, -1.4295e-03,\n",
              "           7.2181e-02, -8.4056e-02,  1.8256e-01, -1.3450e-01, -2.3541e-01,\n",
              "          -6.0954e-02, -1.1661e-01, -5.7155e-02,  3.6646e-03,  1.1190e-01,\n",
              "           1.2386e-01, -1.4012e-01,  2.8506e-01,  1.0120e-01,  1.0247e-01,\n",
              "          -1.0495e-02, -2.1217e-01,  1.4796e-01, -1.6439e-01, -8.1707e-03,\n",
              "           1.0106e-01,  2.8443e-01, -2.2261e-02, -9.2756e-02,  2.3192e-01,\n",
              "           7.3171e-02,  1.7724e-01],\n",
              "         [-1.0942e-01, -4.5649e-02, -1.4472e-01,  4.9949e-02,  1.1348e-01,\n",
              "           2.1573e-02,  8.0419e-02,  7.2992e-02, -3.9718e-02, -1.5485e-01,\n",
              "           5.5787e-03, -1.2712e-01, -1.1023e-01,  1.1435e-02,  3.2039e-03,\n",
              "           1.1138e-01, -1.0020e-01,  1.9830e-01,  4.7746e-02,  1.0768e-01,\n",
              "           4.3526e-02, -2.2720e-01, -1.8552e-02, -1.1626e-01,  4.2308e-02,\n",
              "           9.4284e-02,  1.8369e-01,  1.8344e-03, -1.9798e-02,  2.0976e-01,\n",
              "           5.1398e-05,  2.6377e-03],\n",
              "         [-1.0668e-01, -4.4396e-02, -2.0071e-01,  3.4217e-02,  8.7014e-02,\n",
              "           3.7795e-02,  6.9058e-02,  6.2180e-02, -8.5186e-02, -1.5285e-01,\n",
              "           5.8316e-02, -1.3275e-01, -1.3380e-01, -1.9574e-02,  8.8749e-02,\n",
              "           7.5079e-02, -8.2002e-02,  1.7523e-01,  3.9407e-02,  1.0287e-01,\n",
              "           2.1508e-02, -2.1296e-01, -5.2834e-02, -1.5560e-01, -4.2920e-03,\n",
              "           1.3527e-01,  1.9579e-01, -9.6755e-03, -2.2187e-02,  8.7656e-02,\n",
              "          -1.2590e-02,  3.2658e-02],\n",
              "         [-3.4935e-03, -4.8179e-02, -1.8886e-01,  4.6536e-02, -9.8614e-02,\n",
              "           1.1476e-01, -2.3466e-01,  1.1565e-01, -3.5977e-01, -1.5674e-01,\n",
              "          -1.6096e-01, -1.3988e-01, -2.0687e-01, -6.7339e-02, -2.6518e-02,\n",
              "           7.9617e-02, -1.9369e-01,  2.5837e-01,  2.2222e-01,  7.7403e-02,\n",
              "           1.1297e-01, -3.0814e-02,  4.9266e-02, -2.4036e-01,  2.3053e-02,\n",
              "           8.2456e-02,  4.4319e-01, -9.3879e-02, -1.3721e-01,  3.9742e-01,\n",
              "          -1.8624e-01,  4.5053e-02],\n",
              "         [ 5.9984e-03, -5.5516e-02, -1.9799e-01,  4.4576e-02, -6.1105e-02,\n",
              "           1.0762e-01, -2.3400e-01,  1.1256e-01, -2.9675e-01, -1.7228e-01,\n",
              "          -1.5125e-01, -1.1624e-01, -1.6272e-01, -4.0705e-02, -1.4849e-02,\n",
              "           1.3263e-01, -1.9592e-01,  3.2541e-01,  2.3863e-01,  1.2930e-01,\n",
              "           6.0226e-02, -9.7188e-02,  1.2787e-01, -1.9698e-01,  5.9360e-03,\n",
              "           7.9792e-02,  4.0146e-01, -6.7803e-02, -1.5209e-01,  3.6253e-01,\n",
              "          -1.2402e-01,  5.6599e-02],\n",
              "         [-4.1535e-02, -1.2780e-01, -9.0397e-02,  1.3992e-01, -2.4286e-02,\n",
              "           1.5095e-01, -1.4807e-01,  1.1769e-01, -2.3804e-01, -7.9552e-02,\n",
              "          -2.0751e-01, -1.9620e-01, -2.8993e-01,  2.0420e-02,  3.9658e-02,\n",
              "           2.4495e-01, -8.5454e-02,  2.2709e-01,  1.3161e-01,  2.2787e-01,\n",
              "           1.4992e-01, -3.4336e-02,  9.9222e-03, -1.3985e-01, -1.0166e-01,\n",
              "           4.3003e-02,  2.4215e-01, -6.0195e-03,  8.5914e-02,  2.3675e-01,\n",
              "          -2.0120e-01,  3.4945e-02],\n",
              "         [-8.8848e-02, -8.2538e-02, -1.5301e-01,  1.5548e-01,  4.3822e-02,\n",
              "           1.7030e-01, -2.2186e-01,  8.9041e-02, -2.3608e-01, -4.8504e-02,\n",
              "          -2.1251e-01, -2.0601e-01, -3.6190e-01, -1.1848e-02,  1.1028e-01,\n",
              "           2.3292e-01, -3.5202e-02,  1.9222e-01,  1.4302e-01,  1.8835e-01,\n",
              "           1.3329e-01,  5.2190e-02,  1.4680e-02, -1.4441e-01, -2.8640e-02,\n",
              "           1.0018e-01,  2.8196e-01,  1.3751e-02,  1.0155e-01,  1.9994e-01,\n",
              "          -2.2437e-01,  7.4552e-02],\n",
              "         [-8.5942e-02, -1.3550e-01, -1.0467e-01,  1.6582e-01,  1.0917e-01,\n",
              "          -2.5973e-02, -9.0204e-02,  1.5389e-01,  9.4176e-03,  7.2301e-02,\n",
              "          -3.4749e-02, -5.5078e-02, -2.6885e-01, -5.0681e-02,  3.8878e-02,\n",
              "           2.8229e-01,  1.1529e-02,  1.9400e-02,  3.8533e-02,  1.5419e-01,\n",
              "           1.2848e-01, -8.3570e-02,  8.9742e-02, -2.1329e-01, -1.2123e-03,\n",
              "           1.0061e-01,  1.2967e-01,  9.5389e-02,  1.5212e-01,  2.7369e-01,\n",
              "          -2.2547e-01, -1.0883e-01],\n",
              "         [-8.7181e-02, -1.1244e-01, -1.2259e-01,  1.5199e-01,  7.4023e-02,\n",
              "           3.6412e-02, -7.3549e-02,  1.7345e-01, -4.1197e-02,  4.2258e-02,\n",
              "           2.8091e-02, -6.7016e-02, -3.1547e-01, -1.1426e-01,  6.4736e-02,\n",
              "           2.6108e-01,  3.5603e-02, -1.1013e-03,  2.7920e-02,  1.4200e-01,\n",
              "           1.1580e-01, -5.5462e-02,  1.8850e-02, -1.5808e-01, -2.8172e-02,\n",
              "           1.4793e-01,  1.5023e-01,  8.8696e-02,  1.9596e-01,  2.2341e-01,\n",
              "          -2.1420e-01, -8.7103e-02],\n",
              "         [-6.9998e-02, -1.2051e-01, -1.8595e-01,  1.0253e-01,  1.1474e-01,\n",
              "           1.5391e-01,  5.5371e-02, -5.4774e-03, -1.3661e-01, -4.1015e-02,\n",
              "           7.9446e-03, -2.2171e-01, -3.5184e-01,  7.9619e-02,  5.6709e-02,\n",
              "           1.8644e-01, -9.5300e-03,  2.0144e-01,  2.8810e-03,  2.1725e-01,\n",
              "           4.4526e-02, -7.6300e-02, -1.2075e-01, -9.8469e-02,  2.9778e-02,\n",
              "           1.3623e-01,  1.0471e-01,  5.7933e-03,  8.6043e-02,  5.9760e-02,\n",
              "          -1.0344e-01, -3.4706e-02],\n",
              "         [-9.8764e-02, -1.2045e-01, -3.7650e-02,  1.4628e-01,  1.6199e-01,\n",
              "           6.6568e-02,  9.9568e-02,  8.0182e-02, -5.3196e-03, -5.8792e-02,\n",
              "          -7.0948e-02, -2.2820e-01, -2.3820e-01,  2.8451e-02,  4.4648e-02,\n",
              "           2.2300e-01,  3.6690e-02,  1.2326e-01, -4.7319e-02,  2.0857e-01,\n",
              "           1.2049e-01, -1.0733e-01, -3.1225e-02, -8.2595e-02, -4.3977e-02,\n",
              "           1.4624e-01,  7.6870e-02,  5.2129e-02,  1.4322e-01,  1.1598e-01,\n",
              "          -1.3703e-01, -6.6000e-02]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_size, hidden_size, out_size):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(in_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, out_size)\n",
        "    self.act = nn.GELU()\n",
        "  def forward(self, ins):\n",
        "    hidden = self.act(self.l1(ins))\n",
        "    return self.act(self.l2(hidden))"
      ],
      "metadata": {
        "id": "hW5QG80_K_2K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, n_heads, emb_dim):\n",
        "    super().__init__()\n",
        "    self.layernorm = nn.LayerNorm(emb_dim)\n",
        "    self.n_heads = n_heads\n",
        "    self.emb_dim = emb_dim\n",
        "    self.MHA = MaskedMultiHeadAttention(n_heads, emb_dim)\n",
        "    self.MLP = MLP(emb_dim, emb_dim * 4, emb_dim)\n",
        "  def forward(self, ins):\n",
        "    residual = ins.clone()\n",
        "    attention_result = self.MHA(ins)\n",
        "    attention_result += residual\n",
        "    normalized_attention_result = self.layernorm(attention_result)\n",
        "    residual2 = normalized_attention_result.clone()\n",
        "    MLP_out = self.MLP(normalized_attention_result)\n",
        "    MLP_out += residual2\n",
        "    out = self.layernorm(MLP_out)\n",
        "    return out\n",
        "tb = TransformerBlock(8, emb_dim)\n",
        "tb(torch.randn(1, max_seq_len, emb_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnKYzMWbMPcR",
        "outputId": "9fdc6b20-276f-4f6e-bd50-8851f2c2a863"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.5759e+00, -1.1383e-01,  2.9037e-02, -1.1736e+00, -4.0735e-01,\n",
              "           9.9353e-01, -1.2308e+00, -1.0696e+00, -3.1856e-01,  2.3106e-01,\n",
              "          -4.2182e-02, -3.6915e-01, -1.5488e-03,  9.4527e-01, -3.5165e-01,\n",
              "          -1.5788e+00,  3.0776e-01,  2.3586e+00,  5.7407e-01,  4.4814e-01,\n",
              "           5.9529e-01,  1.5893e+00,  6.3104e-01,  5.5573e-02,  9.0289e-01,\n",
              "           2.2705e+00, -6.8008e-01, -7.0277e-01,  3.7105e-01, -1.7777e+00,\n",
              "          -6.3197e-03, -9.0328e-01],\n",
              "         [ 4.5948e-01,  1.5005e+00, -1.0136e+00,  1.3872e+00,  1.0322e+00,\n",
              "          -1.5098e+00,  1.8345e+00, -2.8755e-01,  4.7084e-02,  1.9839e+00,\n",
              "          -1.4357e+00, -8.3122e-01,  5.3579e-01, -6.0059e-01,  2.7053e-01,\n",
              "          -7.4930e-01,  5.2869e-02,  1.0591e+00, -4.8589e-01, -1.3277e+00,\n",
              "           1.0687e-01, -6.4987e-01,  1.3837e-01,  9.7774e-01, -8.4478e-02,\n",
              "          -4.6687e-01, -7.8971e-01,  1.1028e+00, -7.4115e-01, -2.1027e+00,\n",
              "           4.9262e-01,  9.4646e-02],\n",
              "         [ 1.5239e+00, -1.8936e+00, -7.3335e-02,  1.5540e-01, -1.5368e+00,\n",
              "          -1.8071e-02, -6.5140e-01,  5.1940e-01,  9.0011e-01,  6.1199e-01,\n",
              "           4.6844e-02, -2.3558e-01,  9.0320e-01,  2.6366e-01,  3.4877e-02,\n",
              "          -6.8833e-01,  3.5375e-01,  1.2999e+00, -6.5773e-01, -9.8214e-02,\n",
              "          -9.0048e-01,  9.7742e-01,  1.1329e-01,  2.1237e+00,  8.1678e-01,\n",
              "          -1.4154e-01, -9.2589e-01, -1.5285e+00, -3.4667e-01, -2.1191e+00,\n",
              "           1.7430e+00, -5.7200e-01],\n",
              "         [-1.0377e+00, -4.6105e-02,  3.6616e-02,  7.0784e-01,  8.6201e-01,\n",
              "           2.6940e-01, -1.4340e-01,  7.3082e-01,  2.3965e+00, -5.4552e-01,\n",
              "          -2.0696e-01, -1.5642e-01, -2.2698e+00,  1.5104e+00,  7.6664e-01,\n",
              "          -1.7538e+00,  1.1222e-01,  4.8149e-01, -1.7198e+00,  7.6108e-01,\n",
              "           7.4402e-01,  3.8945e-01, -9.1422e-01, -7.2308e-01, -3.1670e-01,\n",
              "           1.2019e+00, -8.2985e-01, -8.7688e-01, -1.3281e+00,  7.6119e-01,\n",
              "           6.2432e-01,  5.1243e-01],\n",
              "         [ 1.4441e+00,  1.5869e+00, -4.7319e-01,  7.4651e-01,  1.1237e+00,\n",
              "           2.6680e-01,  1.2210e+00, -7.1165e-01, -5.0896e-01,  9.0497e-01,\n",
              "           3.1572e-01,  1.4057e-02, -1.8321e-01,  2.6823e-02, -6.6790e-01,\n",
              "          -6.4501e-01, -1.0314e-01, -1.0690e+00,  3.0873e-01, -2.4951e+00,\n",
              "          -2.1578e+00, -4.4405e-01,  5.6077e-01, -1.5324e+00,  9.8069e-01,\n",
              "          -4.0034e-01,  1.0778e+00, -3.1102e-01,  1.1366e+00, -1.1013e+00,\n",
              "          -6.4100e-02,  1.1531e+00],\n",
              "         [ 8.4706e-01,  5.2915e-01,  1.1804e+00, -1.9576e-01,  1.8915e+00,\n",
              "           4.1866e-01,  9.0260e-01, -8.7054e-01,  8.9758e-01,  6.0993e-01,\n",
              "          -7.6609e-01,  2.5007e-03, -4.8986e-01, -4.5811e-01,  4.2474e-01,\n",
              "          -5.1933e-01, -6.6732e-01, -3.2567e+00,  6.7482e-01,  4.5963e-01,\n",
              "           3.8722e-01, -1.1222e+00,  3.5754e-01, -1.3566e+00, -3.6816e-01,\n",
              "           4.0824e-01,  1.7409e+00,  4.8671e-01,  2.1019e-01, -6.3823e-01,\n",
              "          -1.5891e-01, -1.5616e+00],\n",
              "         [-6.3210e-01, -8.7672e-01,  9.8151e-01,  3.8942e-01,  1.8046e+00,\n",
              "          -9.7451e-01,  1.1140e+00, -1.4970e+00,  9.4106e-01,  6.2812e-01,\n",
              "          -8.2912e-01, -8.1630e-01, -1.9392e-01,  1.0359e+00, -4.3011e-01,\n",
              "          -3.4246e-01, -1.3739e+00, -8.4404e-01,  1.4003e-01, -4.7080e-01,\n",
              "          -1.3665e+00,  1.5429e+00,  6.4313e-01,  1.0397e+00, -2.3739e+00,\n",
              "          -3.4676e-01,  5.1280e-01,  1.1434e+00,  6.8954e-01,  4.2187e-01,\n",
              "           9.9482e-01, -6.5471e-01],\n",
              "         [ 8.3443e-02,  9.4515e-01,  2.7145e-02, -7.5553e-01,  1.4151e+00,\n",
              "           4.4305e-01, -1.0149e+00, -1.0153e-01, -7.5732e-01, -6.0903e-01,\n",
              "           4.4070e-01,  1.3050e+00, -3.3218e-01, -1.2483e+00,  1.9672e-01,\n",
              "          -1.3497e-02, -7.0923e-01, -4.6731e-02,  4.0790e-01, -1.1474e+00,\n",
              "          -9.0280e-01, -4.6729e-01,  3.4672e-01,  1.0895e+00,  1.9763e+00,\n",
              "           1.9443e+00, -1.0784e-01,  1.6778e+00, -4.3158e-01, -8.5059e-01,\n",
              "          -2.6383e+00, -1.6488e-01],\n",
              "         [-2.0668e-01, -1.5666e+00,  1.6546e+00, -1.1646e+00, -9.5443e-01,\n",
              "           9.4831e-01, -2.2973e-01, -8.6671e-01,  7.4882e-02, -3.8996e-01,\n",
              "          -8.6826e-01, -4.1208e-01,  1.3432e-01, -5.6827e-01,  5.1607e-01,\n",
              "           3.4765e-01,  8.5669e-01,  4.7605e-01,  9.8157e-01, -4.0744e-01,\n",
              "           1.6427e+00,  5.1389e-01, -1.0700e+00, -6.6548e-01, -7.7635e-02,\n",
              "           6.3631e-02, -2.9856e-01, -1.2970e+00, -3.3618e-01,  2.7907e+00,\n",
              "          -1.3257e+00,  1.7044e+00],\n",
              "         [-1.3219e+00, -4.3634e-01, -9.9733e-02,  2.6963e+00, -2.0211e+00,\n",
              "          -1.4253e-01,  1.3197e+00,  4.6919e-01, -7.5144e-01, -7.6371e-01,\n",
              "          -4.9766e-01,  1.0925e+00, -1.4604e-01, -1.1907e+00, -7.6556e-01,\n",
              "           8.1217e-01,  5.0498e-01,  4.4553e-01, -3.9224e-01,  2.5850e-01,\n",
              "          -4.0040e-02,  1.0995e+00,  7.3063e-01, -2.7959e-02,  1.6811e-01,\n",
              "          -2.1267e+00,  1.9214e+00, -4.3942e-01,  4.0311e-01,  2.3436e-03,\n",
              "          -1.2196e-01, -6.3878e-01],\n",
              "         [-3.5684e-01, -1.2888e+00, -2.8688e-01, -1.4882e+00, -8.4506e-01,\n",
              "          -3.1756e-01, -1.3842e+00, -6.8582e-01, -2.5487e-01,  1.4842e+00,\n",
              "          -4.7557e-01, -1.2455e+00,  1.0434e+00,  2.1832e+00, -1.9602e-01,\n",
              "           1.0564e+00,  1.2063e-01,  1.9236e+00,  5.5608e-01, -5.8489e-01,\n",
              "          -1.9320e-01, -9.4812e-01, -5.7859e-02, -1.6322e+00,  2.5366e-01,\n",
              "           7.3607e-01,  5.8703e-01, -5.0725e-01, -2.9151e-01,  1.1236e+00,\n",
              "           2.1579e-01,  1.7566e+00],\n",
              "         [ 9.4365e-01,  1.9955e-01, -2.9126e-01,  7.6505e-01,  3.0565e-01,\n",
              "          -3.0112e-01, -1.0722e+00, -1.5426e-01, -3.2509e-01, -1.9443e+00,\n",
              "          -1.3723e+00, -7.2327e-01, -5.6055e-01, -6.9628e-01,  1.1166e+00,\n",
              "          -2.1122e+00, -2.9155e-01,  2.3502e-01, -3.7022e-01, -2.0614e-01,\n",
              "           6.6047e-01,  1.2230e+00,  6.6338e-01,  1.1608e+00,  3.1297e+00,\n",
              "          -6.8655e-01,  1.1993e+00,  2.2174e-02,  1.0212e-01, -5.3060e-01,\n",
              "          -3.6191e-01,  2.7333e-01],\n",
              "         [ 1.7235e+00, -7.0366e-01,  8.7345e-01,  2.9143e-01,  1.7575e+00,\n",
              "          -5.4263e-01,  9.1484e-01, -3.3460e-01, -1.0468e+00,  1.2170e+00,\n",
              "           7.3197e-01,  9.9492e-01, -6.4704e-01,  4.3844e-01, -1.1268e+00,\n",
              "          -5.5014e-01, -5.3400e-01, -1.8886e-01, -7.0226e-01, -1.4736e+00,\n",
              "          -1.6308e+00,  8.2087e-01,  1.0666e+00, -1.9713e+00, -8.8781e-01,\n",
              "          -4.4693e-01,  2.9497e-01,  1.4777e+00,  7.5728e-01, -8.3166e-01,\n",
              "           8.9367e-01, -6.3525e-01],\n",
              "         [-1.8896e-01, -4.4240e-01, -3.7690e-01,  8.6899e-01, -8.1728e-01,\n",
              "          -6.8120e-01,  5.5421e-01, -2.7579e-01, -1.2774e+00,  3.2186e-03,\n",
              "          -1.3107e+00,  1.5058e+00, -1.0527e+00,  9.1601e-01,  4.6803e-01,\n",
              "           1.0643e+00,  2.4325e+00, -6.4199e-01,  6.8758e-01,  4.5519e-01,\n",
              "          -7.6426e-01, -9.4301e-01, -6.1702e-01,  5.4098e-01, -5.2948e-01,\n",
              "          -1.7333e+00,  1.4060e+00, -1.6217e+00,  1.4197e+00,  1.0164e+00,\n",
              "          -4.4586e-01,  3.8096e-01],\n",
              "         [ 1.3020e+00, -8.2752e-02, -2.0599e-01,  3.8976e-01, -2.4110e-01,\n",
              "          -7.0536e-01,  9.1254e-01,  8.9812e-01, -1.3716e-01,  9.7560e-01,\n",
              "          -6.0731e-01, -7.1324e-01, -1.5862e+00,  8.6515e-01, -4.8905e-01,\n",
              "          -2.6630e+00,  1.5097e-02, -2.2094e-01, -1.5438e-01,  2.6109e+00,\n",
              "          -1.0001e+00,  3.8800e-01, -1.8686e+00,  5.8292e-01, -1.6741e-01,\n",
              "           1.2760e+00,  2.1468e-01, -1.7308e-01,  6.1324e-01, -4.6076e-01,\n",
              "           9.8376e-01, -5.5137e-01],\n",
              "         [ 4.4829e-01, -5.3480e-02, -6.2051e-02, -1.5834e-03, -2.2398e+00,\n",
              "          -7.9742e-01,  1.3260e+00, -6.2540e-01,  1.2576e+00, -3.0425e-01,\n",
              "          -1.9698e+00, -5.6058e-01, -1.2142e+00, -1.9571e-01,  5.4720e-01,\n",
              "           1.7261e-01,  1.4770e-01,  1.8426e+00, -1.3241e+00, -2.2625e-01,\n",
              "           2.8444e-01,  4.0767e-01, -6.2740e-01,  1.2178e+00,  2.9720e-01,\n",
              "          -1.1135e+00,  2.5576e-01, -6.6375e-01,  1.8721e+00,  1.4596e+00,\n",
              "          -6.0547e-01,  1.0482e+00]]], grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, n_heads, emb_dim, max_seq_len, n_blocks, tokenizer):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.emb_dim = emb_dim\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.n_blocks = n_blocks\n",
        "    self.tokenizer = tokenizer\n",
        "    self.layernorm = nn.LayerNorm(emb_dim)\n",
        "    self.emb = nn.Embedding(len(tokenizer) + 1, emb_dim)\n",
        "    self.pos_encoding_matrix = create_positional_encoding_matrix(max_seq_len, emb_dim).to(device)\n",
        "    self.decoder_stack = nn.ModuleList([\n",
        "        TransformerBlock(n_heads, emb_dim)\n",
        "        for _ in range(n_blocks)\n",
        "    ])\n",
        "\n",
        "    self.logits_layer = nn.Linear(emb_dim, len(tokenizer)+1)\n",
        "  def forward(self, ins):\n",
        "    embedding = self.emb(ins)\n",
        "    pos_encoding_indices = torch.arange(ins.shape[1]).to(device) # seq_len\n",
        "\n",
        "    x = embedding\n",
        "    x += self.pos_encoding_matrix[pos_encoding_indices]\n",
        "\n",
        "    for decoder in self.decoder_stack:\n",
        "      x = decoder(x)\n",
        "\n",
        "    x = self.layernorm(x)\n",
        "    logits = self.logits_layer(x)\n",
        "    return logits\n",
        "  @torch.no_grad()\n",
        "  def generate(self, text, length=10):\n",
        "    out = \"\"\n",
        "    for x in range(length):\n",
        "      encodings = torch.tensor(self.tokenizer.encode(text + out)).to(device)\n",
        "      seq_len = len(encodings)\n",
        "      if seq_len > self.max_seq_len:\n",
        "        encodings = encodings[len(encodings) - self.max_seq_len:]\n",
        "      tokens = encodings.view(1, -1)\n",
        "      logits = self.forward(tokens)[0, -1, :] # logits for last char\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      selection_index = torch.multinomial(probs, num_samples=1)\n",
        "      out += self.tokenizer.decode([selection_index.item()])\n",
        "    return out"
      ],
      "metadata": {
        "id": "GEt7OmRIPcGP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 10\n",
        "emb_dim = 32\n",
        "n_heads = 8\n",
        "n_blocks = 8\n",
        "\n",
        "lr = 0.001\n",
        "batch_size = 64\n",
        "max_epochs = 1\n",
        "\n",
        "log_period = 100\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "myGPT = GPT(n_heads, emb_dim, max_seq_len, n_blocks, tokenizer)\n",
        "myGPT = myGPT.to(device)\n",
        "print(\"Total params:\", sum(p.numel() for p in myGPT.parameters()))\n",
        "if torch.cuda.is_available():\n",
        "  myGPT = torch.compile(myGPT)\n",
        "optim = torch.optim.AdamW(myGPT.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgwFWjqEiTUr",
        "outputId": "8f7fc93d-ab0c-41a4-ddcc-5add1d49460a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 105474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(max_seq_len, tokens):\n",
        "  context_x = tokens[:max_seq_len]\n",
        "  context_y = tokens[1:max_seq_len+1]\n",
        "  x_examples = [context_x]\n",
        "  y_examples = [context_y]\n",
        "  for token, next in zip(tokens[max_seq_len:], tokens[max_seq_len+1:]):\n",
        "    context_x = context_x[1:]\n",
        "    context_x.append(token)\n",
        "    context_y = context_y[1:]\n",
        "    context_y.append(next)\n",
        "    x_examples.append(context_x)\n",
        "    y_examples.append(context_y)\n",
        "  return torch.tensor(x_examples).long().to(device), torch.tensor(y_examples).to(device)\n",
        "tokenized_train_data = tokenizer.encode(train_text)\n",
        "Xtr, Ytr = generate_dataset(max_seq_len, tokenized_train_data)\n",
        "tokenized_test_data = tokenizer.encode(val_text)\n",
        "Xte, Yte = generate_dataset(max_seq_len, tokenized_test_data)\n",
        "tokenizer.decode(Xtr[0].cpu().numpy()), tokenizer.decode(Ytr[0].cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atTGrKNsfhZn",
        "outputId": "4bee2942-9219-46c8-b80f-9a177064e757"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('First Citi', 'irst Citiz')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_len = len(Xtr) - 1\n",
        "max_steps = (train_len // batch_size) * max_epochs\n",
        "for step in range(1000): # max_steps\n",
        "  batch_indices = torch.randint(0, train_len, (batch_size,)).to(device)\n",
        "  x_batch = Xtr[batch_indices]\n",
        "  y_batch = Ytr[batch_indices].view(-1)\n",
        "\n",
        "  logits = myGPT(x_batch)\n",
        "  logits = logits.view(-1, logits.size(-1))\n",
        "  loss = F.cross_entropy(logits, y_batch)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if step % log_period == 0:\n",
        "    print(\"Step\", str(step) + \", loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5esfI5aiapD",
        "outputId": "41211bcc-7f4e-498f-a47f-55e7dae531d8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, loss: 4.401139259338379\n",
            "Step 100, loss: 2.575706720352173\n",
            "Step 200, loss: 1.4283852577209473\n",
            "Step 300, loss: 0.576152503490448\n",
            "Step 400, loss: 0.4399939179420471\n",
            "Step 500, loss: 0.30750858783721924\n",
            "Step 600, loss: 0.2904627025127411\n",
            "Step 700, loss: 0.2714124023914337\n",
            "Step 800, loss: 0.2468138486146927\n",
            "Step 900, loss: 0.24174949526786804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(myGPT.generate(\"ROMEO:\", length=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eEOVX6D20m3",
        "outputId": "308592dc-bd90-40f1-b3ec-fa4e872da417"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e, he wonra, il! nor,\n",
            "To tog bulwepak?\n",
            "RUKI: th' whesd to anur dadp to isst fon whend hetiin, achis:\n"
          ]
        }
      ]
    }
  ]
}