{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMT0KC4Gp5Y+jKZ53DLK9nN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trickshotblaster/nn-practices/blob/main/actual_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vlOyA0vd1yYH"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BcD5gPE2Kep",
        "outputId": "d13173c1-900a-4fe1-ea14-24833c7201cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: huggingface_hub<0.17,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "NzClVptB12Jp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "trainer = BpeTrainer(vocab_size=1000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
      ],
      "metadata": {
        "id": "A3Rxokcr2IjQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "metadata": {
        "id": "cvh_tXAz2Uzh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij7TOdzN2biA",
        "outputId": "2fb386d6-74c2-4322-fc15-fe0bd05dba8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-02 17:10:06--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.43.120, 52.217.168.120, 52.217.101.118, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.43.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‚Äòwikitext-103-raw-v1.zip.2‚Äô\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  43.8MB/s    in 4.8s    \n",
            "\n",
            "2023-10-02 17:10:11 (38.3 MB/s) - ‚Äòwikitext-103-raw-v1.zip.2‚Äô saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "replace wikitext-103-raw/wiki.test.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "tokenizer.train(files, trainer)"
      ],
      "metadata": {
        "id": "ZBn84zFL2hwB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"tokenizer-wiki.json\")"
      ],
      "metadata": {
        "id": "8QI79bKN2mO5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"tokenizer-wiki.json\")"
      ],
      "metadata": {
        "id": "DwGQU7F92vtI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
        "out_tensor = torch.tensor(output.ids)"
      ],
      "metadata": {
        "id": "Evntl6tB2yD4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quqrI7p2wFZA",
        "outputId": "21c019ca-9afa-41b6-d112-6697e6395b5b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([44, 73, 80, 80, 83, 16, 93, 11, 69, 80, 80,  5, 44, 83, 91, 69, 86, 73,\n",
              "        93, 83, 89,  0, 35])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(out_tensor.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4q1Ct9_X3kGY",
        "outputId": "b4e6c328-7184-492a-f155-f5b4d8443420"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"H e l l o , y ' a l l ! H o w a r e y o u ?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"wikitext-103-raw/wiki.train.raw\", 'r') as f:\n",
        "  text = f.readlines()"
      ],
      "metadata": {
        "id": "NrDLlrAnwHdK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-9nK3Dow41v",
        "outputId": "57408971-f887-4540-9bb2-66aac2562c42"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' \\n',\n",
              " ' = Valkyria Chronicles III = \\n',\n",
              " ' \\n',\n",
              " ' Senj≈ç no Valkyria 3 : Unrecorded Chronicles ( Japanese : Êà¶Â†¥„ÅÆ„É¥„Ç°„É´„Ç≠„É•„É™„Ç¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
              " \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\",\n",
              " \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\",\n",
              " ' \\n',\n",
              " ' = = Gameplay = = \\n',\n",
              " ' \\n',\n",
              " \" As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\"]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "V3gjFH9bz3gY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_len = len(text) - 1"
      ],
      "metadata": {
        "id": "5tSzJD-Az601"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_len = 20\n",
        "emb_dim = 10\n",
        "tokenizer.enable_truncation(context_len)\n",
        "tokenizer.enable_padding(direction='left', length=context_len)"
      ],
      "metadata": {
        "id": "6uMyELPI4yPP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "bP0ZDGN4lj6k"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pretokenize? aaaa but it takes so long"
      ],
      "metadata": {
        "id": "CFe2_6J9qQFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text = torch.tensor([tokenizer.encode(t).ids for t in text])"
      ],
      "metadata": {
        "id": "YDn5YiXTp6Pb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "ZN0LL9AUSXPu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(batch):\n",
        "  xs = []\n",
        "  ys = []\n",
        "  for sentence in batch:\n",
        "    context = \"\"\n",
        "    for word, next in zip(sentence, sentence[1:]):\n",
        "      context += word\n",
        "      xs.append(tokenizer.encode(context).ids)\n",
        "      ys.append(torch.argmax(torch.tensor(tokenizer.encode(word).ids)))\n",
        "  return torch.tensor(xs).to(device), torch.tensor(ys).to(device)"
      ],
      "metadata": {
        "id": "A5SIp0QZ4h3k"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = torch.randn((vocab_size, emb_dim)).to(device)\n",
        "w1 = torch.randn((emb_dim * context_len, 100)).to(device) * 0.1\n",
        "b1 = torch.randn(100).to(device) * 0.01\n",
        "w2 = torch.randn((100, vocab_size)).to(device) * 0.1\n",
        "b2 = torch.randn(vocab_size).to(device) * 0.01\n",
        "\n",
        "params = [C, w1, b1, w2, b2]\n",
        "for p in params:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "vbg43uXNlUts"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "R1tMKevYowwE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 200\n",
        "batch_size = 32\n",
        "lr = 0.05"
      ],
      "metadata": {
        "id": "chfpHaFVSCRK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  batch = [text[random.randint(0, text_len)] for i in range(batch_size)]\n",
        "  #words = torch.tensor([tokenizer.encode_batch(batch)[i].ids for i in range(batch_size)])\n",
        "  x, y = make_batch(batch)\n",
        "\n",
        "  emb = C[x].view(-1, emb_dim * context_len)\n",
        "  l1 = (emb @ w1 + b1).tanh()\n",
        "  out = (l1 @ w2 + b2).tanh()\n",
        "  loss = F.cross_entropy(out, y)\n",
        "\n",
        "  for p in params:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward()\n",
        "  for p in params:\n",
        "    p.data -= p.grad\n",
        "  if epoch % 10 == 0:\n",
        "    print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtqSn50ww9KF",
        "outputId": "8a2a6498-780a-4319-98b6-2bf04cf26dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.878471374511719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def forward(text):\n",
        "  ins = text\n",
        "  x = torch.tensor(tokenizer.encode(text).ids)\n",
        "  emb = C[x].view(-1, emb_dim * context_len)\n",
        "  l1 = (emb @ w1 + b1).tanh()\n",
        "  out = (l1 @ w2 + b2).tanh()\n",
        "  out = out.exp()\n",
        "  out /= out.sum(1, keepdim=True)\n",
        "  sampled = torch.multinomial(out, num_samples=1)\n",
        "  return sampled"
      ],
      "metadata": {
        "id": "1VnyXCycTjSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SNjf5LHmUciy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lol = forward('the')\n",
        "print(lol)\n",
        "tokenizer.decode_batch(lol.cpu().numpy())"
      ],
      "metadata": {
        "id": "m_yv4Ps5URMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(out.cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "YLeccjP3WDFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}