{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNE/mo3WqyW+qDiMh5kRN5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trickshotblaster/nn-practices/blob/main/actual_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vlOyA0vd1yYH"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BcD5gPE2Kep",
        "outputId": "2bea265f-5366-41b7-9874-c3f8ae1317c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub<0.17,>=0.16.4 (from tokenizers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.7.22)\n",
            "Installing collected packages: huggingface_hub, tokenizers\n",
            "Successfully installed huggingface_hub-0.16.4 tokenizers-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "NzClVptB12Jp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
      ],
      "metadata": {
        "id": "A3Rxokcr2IjQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "metadata": {
        "id": "cvh_tXAz2Uzh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij7TOdzN2biA",
        "outputId": "a7d486b9-33b1-4d11-dde9-d36ac32213f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-29 20:10:57--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.138.29, 54.231.229.88, 52.217.140.48, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.138.29|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‚Äòwikitext-103-raw-v1.zip‚Äô\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  58.1MB/s    in 3.2s    \n",
            "\n",
            "2023-09-29 20:11:00 (58.1 MB/s) - ‚Äòwikitext-103-raw-v1.zip‚Äô saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "   creating: wikitext-103-raw/\n",
            "  inflating: wikitext-103-raw/wiki.test.raw  \n",
            "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
            "  inflating: wikitext-103-raw/wiki.train.raw  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "tokenizer.train(files, trainer)"
      ],
      "metadata": {
        "id": "ZBn84zFL2hwB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"tokenizer-wiki.json\")"
      ],
      "metadata": {
        "id": "8QI79bKN2mO5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"tokenizer-wiki.json\")"
      ],
      "metadata": {
        "id": "DwGQU7F92vtI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
        "out_tensor = torch.tensor(output.ids)"
      ],
      "metadata": {
        "id": "Evntl6tB2yD4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quqrI7p2wFZA",
        "outputId": "806eace8-80fd-4377-9a1a-31312dafa700"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([27253,    16,    93,    11,  5097,     5,  7961,  5112,  6218,     0,\n",
              "           35])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(out_tensor.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4q1Ct9_X3kGY",
        "outputId": "0188cb65-77e4-4282-ef69-541e873e07fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello , y ' all ! How are you ?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"wikitext-103-raw/wiki.train.raw\", 'r') as f:\n",
        "  text = f.readlines()"
      ],
      "metadata": {
        "id": "NrDLlrAnwHdK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-9nK3Dow41v",
        "outputId": "b3a8c9ce-998d-4381-84b3-f6235e533a70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' \\n',\n",
              " ' = Valkyria Chronicles III = \\n',\n",
              " ' \\n',\n",
              " ' Senj≈ç no Valkyria 3 : Unrecorded Chronicles ( Japanese : Êà¶Â†¥„ÅÆ„É¥„Ç°„É´„Ç≠„É•„É™„Ç¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
              " \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\",\n",
              " \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\",\n",
              " ' \\n',\n",
              " ' = = Gameplay = = \\n',\n",
              " ' \\n',\n",
              " \" As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\"]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "V3gjFH9bz3gY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_len = len(text) - 1"
      ],
      "metadata": {
        "id": "5tSzJD-Az601"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_len = 20\n",
        "emb_dim = 10\n",
        "tokenizer.enable_truncation(context_len)\n",
        "tokenizer.enable_padding(direction='left', length=context_len)"
      ],
      "metadata": {
        "id": "6uMyELPI4yPP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "bP0ZDGN4lj6k"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pretokenize? aaaa but it takes so long"
      ],
      "metadata": {
        "id": "CFe2_6J9qQFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = torch.tensor([tokenizer.encode(t).ids for t in text])"
      ],
      "metadata": {
        "id": "YDn5YiXTp6Pb"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(batch):\n",
        "  xs = []\n",
        "  ys = []\n",
        "  for sentence in batch:\n",
        "    context = \"\"\n",
        "    for word, next in zip(sentence, sentence[1:]):\n",
        "      context += word\n",
        "      xs.append(tokenizer.encode(context).ids)\n",
        "      ys.append(torch.argmax(torch.tensor(tokenizer.encode(word).ids)))\n",
        "  return torch.tensor(xs), torch.tensor(ys)"
      ],
      "metadata": {
        "id": "A5SIp0QZ4h3k"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = torch.randn((vocab_size, emb_dim))\n",
        "w1 = torch.randn((emb_dim * context_len, 100)) * 0.1\n",
        "b1 = torch.randn(100) * 0.01\n",
        "w2 = torch.randn((100, vocab_size)) * 0.1\n",
        "b2 = torch.randn(vocab_size) * 0.01\n",
        "\n",
        "params = [C, w1, b1, w2, b2]\n",
        "for p in params:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "vbg43uXNlUts"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "R1tMKevYowwE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "batch_size = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  batch = [text[random.randint(0, text_len)] for i in range(batch_size)]\n",
        "  #words = torch.tensor([tokenizer.encode_batch(batch)[i].ids for i in range(batch_size)])\n",
        "  x, y = make_batch(batch)\n",
        "\n",
        "  emb = C[x].view(-1, emb_dim * context_len)\n",
        "  l1 = (emb @ w1 + b1).tanh()\n",
        "  out = (l1 @ w2 + b2).tanh()\n",
        "  loss = F.cross_entropy(out, y)\n",
        "\n",
        "  for p in params:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward()\n",
        "  for p in params:\n",
        "    p.data -= p.grad\n",
        "  if epoch % 10 == 0:\n",
        "    print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtqSn50ww9KF",
        "outputId": "b29f2500-38b9-4013-fc28-6e7a54748d3e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.345673561096191\n",
            "10.530147552490234\n",
            "11.43606185913086\n",
            "10.17839241027832\n",
            "11.347132682800293\n",
            "11.282523155212402\n",
            "10.193549156188965\n",
            "11.276729583740234\n",
            "10.610368728637695\n",
            "10.938106536865234\n",
            "10.547287940979004\n",
            "10.613101959228516\n",
            "9.521679878234863\n",
            "10.457550048828125\n",
            "9.47723388671875\n",
            "10.999757766723633\n",
            "10.536359786987305\n",
            "11.111837387084961\n",
            "10.838302612304688\n",
            "9.468552589416504\n"
          ]
        }
      ]
    }
  ]
}