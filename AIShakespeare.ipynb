{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoh390EpEG5lGYmNTIGBKV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trickshotblaster/nn-practices/blob/main/AIShakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "ERzFbn1Q2CX4"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "text = requests.get(url).text"
      ],
      "metadata": {
        "id": "Lw8FBuTdzMAs"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "vocab_size = len(vocab) + 1\n",
        "block_size = 5\n",
        "stoi = {' ': 0}\n",
        "itos = {0: ' '}\n",
        "stoi.update({char:i+1 for i, char in enumerate(vocab)})\n",
        "itos.update({i+1:char for i, char in enumerate(vocab)})\n",
        "def build_dataset(text, block_size):\n",
        "  xs = []\n",
        "  ys = []\n",
        "  block = [0] * block_size\n",
        "  for char, next in zip(text, text[1:]):\n",
        "    block = block[1:] + [stoi[char]]\n",
        "    xs.append(block)\n",
        "    ys.append(stoi[next])\n",
        "  return xs, ys\n",
        "def make_splits(text, block_size, split_amounts):\n",
        "  train_text = text[0: int(len(text) * split_amounts[0])]\n",
        "  dev_text = text[int(len(text) * split_amounts[0]): int(len(text) * split_amounts[0]) + int(len(text) * split_amounts[1])]\n",
        "  test_text = text[int(len(text) * split_amounts[0]) + int(len(text) * split_amounts[1]):]\n",
        "  Xtr, Ytr = build_dataset(train_text, block_size)\n",
        "  Xdev, Ydev = build_dataset(dev_text, block_size)\n",
        "  Xte, Yte = build_dataset(test_text, block_size)\n",
        "  return torch.tensor(Xtr), torch.tensor(Ytr), torch.tensor(Xdev), torch.tensor(Ydev), torch.tensor(Xte), torch.tensor(Yte)\n",
        "\n",
        "Xtr, Ytr, Xdev, Ydev, Xte, Yte = make_splits(text, block_size, [0.8, 0.1])\n",
        "print(Xtr[0], Ytr[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc7YmXQozjcp",
        "outputId": "f86daeb2-97ac-457a-f821-ad5fa7e8282c"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  0,  0,  0, 19]) tensor(48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 10\n",
        "hidden_size = 200\n",
        "C = torch.randn(vocab_size, emb_dim)\n",
        "w1 = torch.randn(block_size * emb_dim, hidden_size) / ((block_size * emb_dim)**0.5)\n",
        "b1 = torch.randn(hidden_size)\n",
        "w2 = torch.randn(hidden_size, vocab_size) / (hidden_size ** 0.5)\n",
        "b2 = torch.randn(vocab_size)\n",
        "\n",
        "params = [C, w1, b1, w2, b2]\n",
        "for p in params:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "d41x4PpI3EuQ"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "ins = C[Xtr[20]].view(-1)\n",
        "l1 = (ins @ w1 + b1).tanh()\n",
        "out = F.softmax(l1 @ w2 + b2, dim=0)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQC63eGZ4oRC",
        "outputId": "c0151a86-100e-4d3f-cab9-acb953135cb2"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0005, 0.0069, 0.0038, 0.0391, 0.0113, 0.0126, 0.0020, 0.0181, 0.0015,\n",
            "        0.0095, 0.0008, 0.0145, 0.0132, 0.0022, 0.0434, 0.0064, 0.0072, 0.0014,\n",
            "        0.0635, 0.0716, 0.0090, 0.0123, 0.0096, 0.0038, 0.0157, 0.0124, 0.0158,\n",
            "        0.0038, 0.0012, 0.0055, 0.0031, 0.0010, 0.0036, 0.0028, 0.1118, 0.0038,\n",
            "        0.0059, 0.0019, 0.0059, 0.0200, 0.0106, 0.0110, 0.0115, 0.0157, 0.0089,\n",
            "        0.0013, 0.0039, 0.0353, 0.0029, 0.0042, 0.0125, 0.0008, 0.0125, 0.0270,\n",
            "        0.0093, 0.0070, 0.0698, 0.0026, 0.0064, 0.0362, 0.0012, 0.0433, 0.0516,\n",
            "        0.0190, 0.0165, 0.0006], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "batch_size = 16\n",
        "train_len = len(Xtr) - 1\n",
        "max_steps = (train_len // batch_size) * num_epochs\n",
        "lr = 1"
      ],
      "metadata": {
        "id": "FIDG2Obr20yL"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(max_steps):\n",
        "  batch_indices = torch.randint(0, train_len, (batch_size,))\n",
        "  x_batch = Xtr[batch_indices]\n",
        "  y_batch = Ytr[batch_indices]\n",
        "  emb = C[x_batch].view(batch_size, -1)\n",
        "  l1 = (emb @ w1 + b1).tanh()\n",
        "\n",
        "  out = F.softmax(l1 @ w2 + b2, dim=1)\n",
        "\n",
        "  loss = -out[torch.arange(batch_size), y_batch].log().mean()\n",
        "\n",
        "\n",
        "  for p in params:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  for p in params:\n",
        "    p.data -= p.grad * lr\n",
        "\n",
        "  if step % 10000 == 0:\n",
        "    print(\"Step\", str(step) + \", loss:\", loss.item())\n",
        "  elif step % 100 == 0:\n",
        "    with torch.no_grad():\n",
        "      batch_indices = torch.randint(0, train_len, (64,))\n",
        "      x_batch = Xtr[batch_indices]\n",
        "      y_batch = Ytr[batch_indices]\n",
        "      emb = C[x_batch].view(64, -1)\n",
        "      l1 = (emb @ w1 + b1).tanh()\n",
        "\n",
        "      out = F.softmax(l1 @ w2 + b2, dim=1)\n",
        "\n",
        "      loss = -out[torch.arange(64), y_batch].log().mean()\n",
        "\n",
        "      val_batch_indices = torch.randint(0, len(Xdev) - 1, (64,))\n",
        "      val_batch = Xdev[val_batch_indices]\n",
        "      val_ys = Ydev[val_batch_indices]\n",
        "      val_emb = C[val_batch].view(64, -1)\n",
        "      val_l1 = (val_emb @ w1 + b1).tanh()\n",
        "      val_out = (val_l1 @ w2 + b2).softmax(dim=1)\n",
        "      val_loss = -val_out[torch.arange(64), val_ys].log().mean()\n",
        "      if (val_loss - loss) > (loss * (1/((step+1)/10000))):\n",
        "        print(\"Stopping with train loss\", loss, \"and val loss\", val_loss)\n",
        "        break\n",
        "    lr *= 0.9"
      ],
      "metadata": {
        "id": "_sAd07Zv43l0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50876723-026e-4222-9f0a-34103c16172b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, loss: 4.631681442260742\n",
            "Step 10000, loss: 2.2440900802612305\n",
            "Step 20000, loss: 2.6413097381591797\n",
            "Step 30000, loss: 2.8962361812591553\n",
            "Stopping with train loss tensor(1.9298) and val loss tensor(2.5415)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate():\n",
        "  out_str = \"\"\n",
        "  context = [0] * block_size\n",
        "  for x in range(500):\n",
        "    c = torch.tensor(context)\n",
        "    emb = C[c].view(-1)\n",
        "    l1 = l1 = (emb @ w1 + b1).tanh()\n",
        "    out = F.softmax(l1 @ w2 + b2, dim=0)\n",
        "    index = torch.multinomial(out, num_samples=1)\n",
        "    context = context[1:] + [index.item()]\n",
        "    out_str += itos[index.item()]\n",
        "  return out_str\n",
        "print(generate())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2Yy_5DyWwAC",
        "outputId": "d329ea5c-10b6-459e-f9ee-6b06cc3c9741"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ",\n",
            "K:\n",
            "The corsurs shist nuls ghee by,\n",
            "RMmRIOK:\n",
            "IThe patr andserisd:\n",
            "He Wheln d poke ney, and l wath rrer some!s-he illo watf tour anel areld of ing ame fouthen's sald alveng're chock of ime th maen'd preriogs!\n",
            "Juar ofparnome,\n",
            "Am, kof ther for tig to co wesseskir wiithery che, s istarnow thins gessren fhime bank\n",
            "Thee his unpoun kiee outh;\n",
            "I wh aldat ogitt out.\n",
            "\n",
            "SANI:\n",
            "Shal m:\n",
            "Koe ofore, doderin do kerb.\n",
            "SThe p fosal Marlo fot luvenveoff,\n",
            "Mitaa, de'dy m athet :ott,\n",
            "kid, bye kyout fe ye save thy, fie\n"
          ]
        }
      ]
    }
  ]
}