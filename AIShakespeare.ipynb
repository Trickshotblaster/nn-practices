{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGRGPzLp8exRYbfvxI8s64",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trickshotblaster/nn-practices/blob/main/AIShakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ERzFbn1Q2CX4"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "text = requests.get(url).text"
      ],
      "metadata": {
        "id": "Lw8FBuTdzMAs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "vocab_size = len(vocab) + 1\n",
        "block_size = 5\n",
        "stoi = {' ': 0}\n",
        "itos = {0: ' '}\n",
        "stoi.update({char:i+1 for i, char in enumerate(vocab)})\n",
        "itos.update({i+1:char for i, char in enumerate(vocab)})\n",
        "def build_dataset(text, block_size):\n",
        "  xs = []\n",
        "  ys = []\n",
        "  block = [0] * block_size\n",
        "  for char, next in zip(text, text[1:]):\n",
        "    block = block[1:] + [stoi[char]]\n",
        "    xs.append(block)\n",
        "    ys.append(stoi[next])\n",
        "  return xs, ys\n",
        "def make_splits(text, block_size, split_amounts):\n",
        "  train_text = text[0: int(len(text) * split_amounts[0])]\n",
        "  dev_text = text[int(len(text) * split_amounts[0]): int(len(text) * split_amounts[0]) + int(len(text) * split_amounts[1])]\n",
        "  test_text = text[int(len(text) * split_amounts[0]) + int(len(text) * split_amounts[1]):]\n",
        "  Xtr, Ytr = build_dataset(train_text, block_size)\n",
        "  Xdev, Ydev = build_dataset(dev_text, block_size)\n",
        "  Xte, Yte = build_dataset(test_text, block_size)\n",
        "  return torch.tensor(Xtr), torch.tensor(Ytr), torch.tensor(Xdev), torch.tensor(Ydev), torch.tensor(Xte), torch.tensor(Yte)\n",
        "\n",
        "Xtr, Ytr, Xdev, Ydev, Xte, Yte = make_splits(text, block_size, [0.8, 0.1])\n",
        "print(Xtr[0], Ytr[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc7YmXQozjcp",
        "outputId": "73eeb043-0ad5-4132-976a-2e7768ffa316"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  0,  0,  0, 19]) tensor(48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 10\n",
        "hidden_size = 100\n",
        "C = torch.randn(vocab_size, emb_dim)\n",
        "w1 = torch.randn(block_size * emb_dim, hidden_size) / ((block_size * emb_dim)**0.5)\n",
        "b1 = torch.randn(hidden_size)\n",
        "w2 = torch.randn(hidden_size, vocab_size) / (hidden_size ** 0.5)\n",
        "b2 = torch.randn(vocab_size)\n",
        "\n",
        "params = [C, w1, b1, w2, b2]\n",
        "for p in params:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "d41x4PpI3EuQ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "ins = C[Xtr[20]].view(-1)\n",
        "l1 = (ins @ w1 + b1).tanh()\n",
        "out = F.softmax(l1 @ w2 + b2, dim=0)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQC63eGZ4oRC",
        "outputId": "5a3a845b-2621-41e0-e579-3b43794b2d26"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0004, 0.0114, 0.0107, 0.0050, 0.0093, 0.0024, 0.0151, 0.0041, 0.0077,\n",
            "        0.0087, 0.0026, 0.0060, 0.0020, 0.0076, 0.0825, 0.0041, 0.0003, 0.0013,\n",
            "        0.0009, 0.0128, 0.0152, 0.0008, 0.0181, 0.0225, 0.0248, 0.0025, 0.0270,\n",
            "        0.0051, 0.2682, 0.0288, 0.0073, 0.0027, 0.0077, 0.0037, 0.0022, 0.0014,\n",
            "        0.0013, 0.0026, 0.0025, 0.0060, 0.0162, 0.0054, 0.0047, 0.0011, 0.0205,\n",
            "        0.0035, 0.0062, 0.0019, 0.0383, 0.0369, 0.0020, 0.0927, 0.0376, 0.0024,\n",
            "        0.0072, 0.0056, 0.0007, 0.0004, 0.0007, 0.0183, 0.0209, 0.0086, 0.0095,\n",
            "        0.0020, 0.0043, 0.0068], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "batch_size = 16\n",
        "train_len = len(Xtr) - 1\n",
        "max_steps = (train_len // batch_size) * num_epochs\n",
        "lr = 1"
      ],
      "metadata": {
        "id": "FIDG2Obr20yL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(max_steps):\n",
        "  batch_indices = torch.randint(0, train_len, (batch_size,))\n",
        "  x_batch = Xtr[batch_indices]\n",
        "  y_batch = Ytr[batch_indices]\n",
        "  emb = C[x_batch].view(batch_size, -1)\n",
        "  l1 = (emb @ w1 + b1).tanh()\n",
        "\n",
        "  out = F.softmax(l1 @ w2 + b2, dim=1)\n",
        "\n",
        "  loss = -out[torch.arange(batch_size), y_batch].log().mean()\n",
        "\n",
        "\n",
        "  for p in params:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  for p in params:\n",
        "    p.data -= p.grad * lr\n",
        "\n",
        "  if step % 10000 == 0:\n",
        "    print(\"Step\", str(step) + \", loss:\", loss.item())\n",
        "  elif step % 100 == 0:\n",
        "    lr *= 0.9"
      ],
      "metadata": {
        "id": "_sAd07Zv43l0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f577db66-2e8a-4e39-cdfe-d2599a605020"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, loss: 4.656942367553711\n",
            "Step 10000, loss: 2.4907734394073486\n",
            "Step 20000, loss: 1.818792700767517\n",
            "Step 30000, loss: 1.6171382665634155\n",
            "Step 40000, loss: 1.6939035654067993\n",
            "Step 50000, loss: 1.475493311882019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate():\n",
        "  out_str = \"\"\n",
        "  context = [0] * block_size\n",
        "  for x in range(500):\n",
        "    c = torch.tensor(context)\n",
        "    emb = C[c].view(-1)\n",
        "    l1 = l1 = (emb @ w1 + b1).tanh()\n",
        "    out = F.softmax(l1 @ w2 + b2, dim=0)\n",
        "    index = torch.multinomial(out, num_samples=1)\n",
        "    context = context[1:] + [index.item()]\n",
        "    out_str += itos[index.item()]\n",
        "  return out_str\n",
        "print(generate())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2Yy_5DyWwAC",
        "outputId": "112dacc0-c7f9-4251-d84b-3daefe7d92e8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SEMS:\n",
            "Nat trourss,\n",
            "Aspirese; brourannud, and robral!\n",
            "\n",
            "IUOLIUES:\n",
            "Whef ly Fith as alice\n",
            "Tfu to thau and that, selint that eolest stip enetthin this youigt toke we istreat thILwer on arst thegpertitibs glock thk re hing one and tas thiaist in mastodr: I hur,\n",
            "Oig on nos mak, sor!\n",
            "\n",
            "Dock Vom Hin sicag fo fons then wiag beoti love? rol,\n",
            "Sh\n",
            "ES:\n",
            "Whouls sudl soan dacatius fuptirizy'et Mingond ser to akave ase.\n",
            "I wore make of herstre be groe melprine:\n",
            "\n",
            "ell on moc sI\n",
            "And lore, loond thou rt lowe:\n",
            "Doue and \n"
          ]
        }
      ]
    }
  ]
}